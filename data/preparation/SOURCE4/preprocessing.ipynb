{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr7OF62xt-5H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Datasets/SOURCE5/SOURCE4.zip', 'r') as z_f:\n",
        "  z_f.extractall('/content')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install english-words"
      ],
      "metadata": {
        "id": "gD5z3uE_ufzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "freq_df = pd.read_csv('/content/drive/MyDrive/Datasets/etc/unigram_freq.csv')\n",
        "freq_df = freq_df.sort_values(by=['count'], axis=0, ascending=False)\n",
        "freq_words = freq_df['word'].to_numpy()\n",
        "freq_df.head()"
      ],
      "metadata": {
        "id": "lzxt7zVuwv2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from english_words import get_english_words_set\n",
        "import numpy as np\n",
        "\n",
        "words_list = list(get_english_words_set(['web2']))\n",
        "\n",
        "SOURCE2_WORDS_PATH = '/content/drive/MyDrive/Datasets/SOURCE2/words_new.txt'\n",
        "words_txt = []\n",
        "with open(SOURCE2_WORDS_PATH, 'r') as w_f:\n",
        "    lines = w_f.readlines()\n",
        "    for line in lines:\n",
        "        if line[0] == \"#\":\n",
        "            continue\n",
        "        spl = line.strip().split(\" \")\n",
        "        words_txt.append(spl[-1])\n",
        "\n",
        "unique_source2_words = np.unique(np.array(words_txt))\n",
        "freq_without_source2 = [w for w in freq_words if w not in unique_source2_words]\n",
        "freq_source2_intersection = [w for w in freq_words if w in unique_source2_words]\n"
      ],
      "metadata": {
        "id": "4U7P5i08umtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vocab_size = 50250\n",
        "\n",
        "freq_new_size = model_vocab_size - len(unique_source2_words)\n",
        "cropped_freq = freq_without_source2[:freq_new_size]\n",
        "\n",
        "unique_source2_words, source2_counts = np.unique(words_txt, return_counts=True)\n",
        "sorted_pairs = sorted(zip(unique_source2_words, source2_counts), key=lambda x: x[1], reverse=True)\n",
        "sorted_unique, sorted_counts = zip(*sorted_pairs)\n",
        "\n",
        "sorted_unique = list(sorted_unique)\n",
        "sorted_counts = list(sorted_counts)\n",
        "\n",
        "total_counts = sum(sorted_counts)\n",
        "probabilities = [count / total_counts for count in sorted_counts]"
      ],
      "metadata": {
        "id": "_Qd9Mulluo53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/words'\n",
        "DATA_WRITERS_DIRS = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]"
      ],
      "metadata": {
        "id": "bPAaizGfwX3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WRITERS_DATA_IN_VOCAB = {}\n",
        "\n",
        "for d in DATA_WRITERS_DIRS:\n",
        "  writer_dir = os.path.join(DATA_DIR, d)\n",
        "  writer_sources_dirs = [d for d in os.listdir(writer_dir) if os.path.isdir(os.path.join(writer_dir, d))]\n",
        "\n",
        "  writer_valid_data = []\n",
        "\n",
        "  for source_d in writer_sources_dirs:\n",
        "    contents_file_path = os.path.join(writer_dir, source_d + '.txt')\n",
        "\n",
        "    with open(contents_file_path, 'r') as c_f:\n",
        "      lines = c_f.readlines()\n",
        "      for line in lines:\n",
        "        splitted = line.strip().split()\n",
        "\n",
        "        if splitted[-1] in cropped_freq or splitted[-1] in sorted_unique:\n",
        "          writer_valid_data.append([splitted[0], splitted[-1]])\n",
        "\n",
        "  WRITERS_DATA_IN_VOCAB[d] = writer_valid_data"
      ],
      "metadata": {
        "id": "zciqm9LjxHsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([len(WRITERS_DATA_IN_VOCAB[d]) for d in [*WRITERS_DATA_IN_VOCAB]])"
      ],
      "metadata": {
        "id": "UPfpieN4CNTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(WRITERS_DATA_IN_VOCAB[\"0\"])"
      ],
      "metadata": {
        "id": "xnCp7FXQCapy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_count = sum([len(WRITERS_DATA_IN_VOCAB[d]) for d in [*WRITERS_DATA_IN_VOCAB]])\n",
        "print(valid_count, int(valid_count*0.13), int(valid_count*0.21), int(valid_count*0.13)/640, int(valid_count*0.20)/640)"
      ],
      "metadata": {
        "id": "VLGpS6j_2yNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_writers_n = 8\n",
        "validation_writers_n = 5"
      ],
      "metadata": {
        "id": "HfGrSfqjDdzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OVERALL_COUNT = 0\n",
        "for d in DATA_WRITERS_DIRS:\n",
        "    writer_sources_dirs = [d for d in os.listdir(writer_dir) if os.path.isdir(os.path.join(writer_dir, d))]\n",
        "    for src_d in writer_sources_dirs:\n",
        "      OVERALL_COUNT += len(os.listdir(os.path.join(DATA_DIR, d, src_d)))\n",
        "\n",
        "print(f\"Overall count: {OVERALL_COUNT}\")\n",
        "print(f\"Percentage of data that intersects with model dict: {(valid_count/OVERALL_COUNT) * 100.0:.2f}%\")"
      ],
      "metadata": {
        "id": "7_PlAP-r5gb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "WRITERS_TEST_DATA = {}\n",
        "WRITERS_VALIDATION_DATA = {}\n",
        "WRITERS_TRAIN_DATA = {}\n",
        "\n",
        "for d in [*WRITERS_DATA_IN_VOCAB][:test_writers_n]:\n",
        "  WRITERS_TEST_DATA[d] = []\n",
        "  for fp, transcription in WRITERS_DATA_IN_VOCAB[d]:\n",
        "      WRITERS_TEST_DATA[d].append([fp, transcription])\n",
        "\n",
        "for d in [*WRITERS_DATA_IN_VOCAB][:test_writers_n + validation_writers_n]:\n",
        "  WRITERS_VALIDATION_DATA[d] = []\n",
        "  for fp, transcription in WRITERS_DATA_IN_VOCAB[d]:\n",
        "      WRITERS_VALIDATION_DATA[d].append([fp, transcription])\n",
        "\n",
        "for d in [*WRITERS_DATA_IN_VOCAB][test_writers_n + validation_writers_n:]:\n",
        "  WRITERS_TRAIN_DATA[d] = []\n",
        "  for fp, transcription in WRITERS_DATA_IN_VOCAB[d]:\n",
        "      WRITERS_TRAIN_DATA[d].append([fp, transcription])"
      ],
      "metadata": {
        "id": "r62ZsbJv26jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "def generate_filename(frmt='jpg'):\n",
        "    return str(time.time()) + '_' + str(random.randint(100, 999)) + '.' + frmt\n",
        "\n",
        "out_train_dir = '/content/SOURCE4/train'\n",
        "out_validation_dir = '/content/SOURCE4/validation'\n",
        "out_test_dir = '/content/SOURCE4/test'\n",
        "\n",
        "train_csv = '/content/SOURCE4/train.csv'\n",
        "validation_csv = '/content/SOURCE4/validation.csv'\n",
        "test_csv = '/content/SOURCE4/test.csv'\n",
        "\n",
        "os.makedirs(out_train_dir, exist_ok=True)\n",
        "os.makedirs(out_validation_dir, exist_ok=True)\n",
        "os.makedirs(out_test_dir, exist_ok=True)\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "\n",
        "def process_images(writers_data, out_dir):\n",
        "  for w_dir in [*writers_data]:\n",
        "    for fp, transcription in writers_data[w_dir]:\n",
        "      src_path = os.path.join(DATA_DIR, w_dir, fp)\n",
        "\n",
        "      with Image.open(src_path) as img:\n",
        "          out_filename = generate_filename()\n",
        "          out_path = os.path.join(out_dir, out_filename)\n",
        "\n",
        "          while os.path.exists(out_path):\n",
        "              out_path = os.path.join(out_dir, generate_filename())\n",
        "\n",
        "          img.convert('RGB').save(out_path, 'JPEG')\n",
        "\n",
        "          filenames_list.append(out_filename)\n",
        "          transcriptions_list.append(transcription)\n",
        "\n",
        "def save_to_csv_and_archive(filenames_list, transcriptions_list, csv_path, archive_path, out_dir):\n",
        "    df = pd.DataFrame({\n",
        "        'filename': filenames_list,\n",
        "        'transcription': transcriptions_list\n",
        "    })\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    shutil.make_archive(archive_path, \"zip\", out_dir)\n",
        "\n",
        "\n",
        "process_images(WRITERS_TEST_DATA, out_test_dir)\n",
        "save_to_csv_and_archive(filenames_list, transcriptions_list, test_csv, \"/content/SOURCE4/test\", out_test_dir)\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "\n",
        "process_images(WRITERS_VALIDATION_DATA, out_validation_dir)\n",
        "save_to_csv_and_archive(filenames_list, transcriptions_list, validation_csv, \"/content/SOURCE4/validation\", out_validation_dir)\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "\n",
        "process_images(WRITERS_TRAIN_DATA, out_train_dir)\n",
        "save_to_csv_and_archive(filenames_list, transcriptions_list, train_csv, \"/content/SOURCE4/train\", out_train_dir)"
      ],
      "metadata": {
        "id": "JeermyrK5LES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}