{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zuo1F6fJ04Sy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "SOURCE2_DIR = '/content/source2-recognition'\n",
        "source2_test_annotations = os.path.join(SOURCE2_DIR, 'test_annotations.json')\n",
        "source2_train_annotations = os.path.join(SOURCE2_DIR, 'train_annotations.json')\n",
        "source2_validation_annotations = os.path.join(SOURCE2_DIR, 'validation_annotations.json')\n",
        "\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Datasets/SOURCE2/recognition.zip\", \"r\") as f:\n",
        "  f.extractall(SOURCE2_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "\n",
        "def process_images(source_dir, out_dir, mode=\"train\"):\n",
        "    json_path = os.path.join(source_dir, mode + \"_annotations.json\")\n",
        "    with open(json_path, 'r') as f:\n",
        "        annotations = json.loads(f.read())\n",
        "\n",
        "    words_dir = os.path.join(source_dir, \"words\")\n",
        "    word_dirs = [d for d in os.listdir(words_dir) if os.path.isdir(os.path.join(words_dir, d))]\n",
        "\n",
        "    for page_id in [*annotations]:\n",
        "        pages_dir, _ = page_id.split(\"-\")\n",
        "        page_words_dir = os.path.join(words_dir, pages_dir, page_id)\n",
        "        page_words_paths = [os.path.join(page_words_dir, f) for f in os.listdir(page_words_dir)\n",
        "         if os.path.isfile(os.path.join(page_words_dir, f))\n",
        "         and f[:-4] != \"r06-022-03-05\" and f[:-4] != \"a01-117-05-02\"]\n",
        "        page_words_ids = [os.path.basename(p)[:-4] for p in page_words_paths]\n",
        "        for word_dict in annotations[page_id][\"words\"]:\n",
        "            word_id = word_dict[\"word_id\"]\n",
        "            if word_id in page_words_ids:\n",
        "              out_path = os.path.join(out_dir, mode, word_id + '.jpg')\n",
        "              os.makedirs(os.path.join(out_dir, mode), exist_ok=True)\n",
        "\n",
        "              word_img = Image.open(page_words_paths[page_words_ids.index(word_id)]).convert(\"RGB\")\n",
        "              word_img.save(out_path)\n",
        "\n",
        "              filenames_list.append(word_id + '.jpg')\n",
        "              transcriptions_list.append(word_dict[\"transcription\"])\n",
        "            else:\n",
        "                print(f\"{word_id} is not in data\")\n",
        "\n",
        "def save_to_csv_and_archive(filenames_list, transcriptions_list, csv_path, archive_path, out_dir):\n",
        "    df = pd.DataFrame({\n",
        "        'filename': filenames_list,\n",
        "        'transcription': transcriptions_list\n",
        "    })\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    shutil.make_archive(archive_path, \"zip\", out_dir)\n"
      ],
      "metadata": {
        "id": "NWXXLFFN1ZNR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "process_images(SOURCE2_DIR, \"/content/SOURCE2/\", \"test\")\n",
        "save_to_csv_and_archive(filenames_list, transcriptions_list, \"/content/SOURCE2/test.csv\", \"/content/SOURCE2/test\", \"/content/SOURCE2/test\")\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n"
      ],
      "metadata": {
        "id": "5GLPqV0u1Zce"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_images(SOURCE2_DIR, \"/content/SOURCE2/\", \"validation\")\n",
        "save_to_csv_and_archive(filenames_list, transcriptions_list, \"/content/SOURCE2/validation.csv\", \"/content/SOURCE2/validation\", \"/content/SOURCE2/validation\")\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "\n",
        "process_images(SOURCE2_DIR, \"/content/SOURCE2/\", \"train\")\n",
        "save_to_csv_and_archive(filenames_list, transcriptions_list, \"/content/SOURCE2/train.csv\", \"/content/SOURCE2/train\", \"/content/SOURCE2/train\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZhqRTJQSBm5",
        "outputId": "0010e84f-c552-4666-bb07-f9878a3c2874"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a01-117-05-02 is not in data\n",
            "r06-022-03-05 is not in data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_path = os.path.join(\"/content/source2-recognition/test_annotations.json\")\n",
        "with open(json_path, 'r') as f:\n",
        "    test_annotations = json.loads(f.read())\n",
        "\n",
        "json_path = os.path.join(\"/content/source2-recognition/train_annotations.json\")\n",
        "with open(json_path, 'r') as f:\n",
        "    train_annotations = json.loads(f.read())\n",
        "\n",
        "json_path = os.path.join(\"/content/source2-recognition/validation_annotations.json\")\n",
        "with open(json_path, 'r') as f:\n",
        "    validation_annotations = json.loads(f.read())\n",
        "\n",
        "all_annot_ids = [*test_annotations] + [*train_annotations] + [*validation_annotations]"
      ],
      "metadata": {
        "id": "Qidn4vxFM8Ec"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_dir = \"/content/source2-recognition/words\"\n",
        "word_dirs = [d for d in os.listdir(words_dir) if os.path.isdir(os.path.join(words_dir, d))]\n",
        "\n",
        "all_words_data_ids = []\n",
        "\n",
        "for dir in word_dirs:\n",
        "  word_subdirs = [subdir for subdir in os.listdir(\n",
        "      os.path.join(words_dir, dir))\n",
        "  if os.path.isdir(os.path.join(words_dir, dir, subdir))]\n",
        "  all_words_data_ids += word_subdirs"
      ],
      "metadata": {
        "id": "FFYRt11CSCH9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([id for id in all_words_data_ids if id not in all_annot_ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaBALTaucYrl",
        "outputId": "0ccdf506-2991-4159-8aef-a13d85654af4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fohswJg1lVZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}