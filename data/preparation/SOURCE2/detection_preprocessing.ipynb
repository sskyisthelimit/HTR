{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rld0qaOpEkv9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Datasets/SOURCE2/detection/pages.zip' ,'r') as f:\n",
        "  f.extractall('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/Datasets/SOURCE2/detection/xml.zip', 'r') as f:\n",
        "  f.extractall('/content/')"
      ],
      "metadata": {
        "id": "xt_TNtJ0FNTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pages_ids = []\n",
        "with open('/content/drive/MyDrive/Datasets/SOURCE2/splits/test.uttlist', 'r') as split_f:\n",
        "  test_pages_ids = [line.strip().replace('\\n', '') for line in split_f.readlines()]\n",
        "\n",
        "train_pages_ids = []\n",
        "with open('/content/drive/MyDrive/Datasets/SOURCE2/splits/train.uttlist', 'r') as split_f:\n",
        "  train_pages_ids = [line.strip().replace('\\n', '') for line in split_f.readlines()]\n",
        "\n",
        "validation_pages_ids = []\n",
        "with open('/content/drive/MyDrive/Datasets/SOURCE2/splits/validation.uttlist', 'r') as split_f:\n",
        "  validation_pages_ids = [line.strip().replace('\\n', '') for line in split_f.readlines()]\n",
        "\n",
        "print(test_pages_ids)\n",
        "print(train_pages_ids)\n",
        "print(validation_pages_ids)"
      ],
      "metadata": {
        "id": "eJ5x5sDVF8Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_nonhidden_dirs(path):\n",
        "    return [f for f in os.listdir(path) if not f.startswith('.') and os.path.isdir(os.path.join(path, f))]\n",
        "\n",
        "def list_png_files(path):\n",
        "    return [f for f in os.listdir(path) if f.endswith('.png')]\n",
        "\n",
        "def find_and_append_files(ids, files_paths, dir, filename, data_dir):\n",
        "    matches = [id for id in ids if id == filename]\n",
        "    if len(matches) > 1:\n",
        "        print(matches)\n",
        "        raise ValueError(f'More than one match occurred for {filename}.')\n",
        "    elif matches:\n",
        "        files_paths.append(os.path.join(data_dir, dir, filename + '.png'))\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def process_files(data_dir, test_ids, train_ids, validation_ids):\n",
        "    test_files, train_files, validation_files = [], [], []\n",
        "\n",
        "    for dir in list_nonhidden_dirs(data_dir):\n",
        "        dir_path = os.path.join(data_dir, dir)\n",
        "        for filename in list_png_files(dir_path):\n",
        "            is_test = find_and_append_files(\n",
        "                test_ids, test_files, dir, filename[:-4], data_dir)\n",
        "            is_train = find_and_append_files(\n",
        "                train_ids, train_files, dir, filename[:-4], data_dir)\n",
        "            is_validation = find_and_append_files(\n",
        "                validation_ids, validation_files, dir, filename[:-4], data_dir)\n",
        "\n",
        "            if not (is_test or is_train or is_validation):\n",
        "                train_files.append(os.path.join(data_dir, dir, filename))\n",
        "                print(f\"No match found for {filename}, added to train set.\")\n",
        "\n",
        "    return test_files, train_files, validation_files\n",
        "\n",
        "pages_data_dir = '/content/data'\n",
        "\n",
        "test_files_paths, train_files_paths, validation_files_paths = process_files(\n",
        "    pages_data_dir, test_pages_ids, train_pages_ids, validation_pages_ids\n",
        ")\n"
      ],
      "metadata": {
        "id": "zQFvEOpjGtM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_pages_ids), len(test_files_paths))\n",
        "print(len(train_pages_ids), len(train_files_paths))\n",
        "print(len(validation_pages_ids), len(validation_files_paths))"
      ],
      "metadata": {
        "id": "lOUa2-cgI3N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len([f for f in os.listdir('/content/xml')]),\n",
        "      len(test_pages_ids) + len(train_pages_ids) + len(validation_pages_ids))"
      ],
      "metadata": {
        "id": "eMygSlkJskaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_train_dir = '/content/SOURCE2/train'\n",
        "out_test_dir = '/content/SOURCE2/test'\n",
        "out_validation_dir = '/content/SOURCE2/validation'\n",
        "\n",
        "os.makedirs(out_train_dir, exist_ok=True)\n",
        "os.makedirs(out_test_dir, exist_ok=True)\n",
        "os.makedirs(out_validation_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "V6Ee5RG6HptR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def parse_iam_annotation(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    print(root.attrib['id'])\n",
        "    form_info = {\n",
        "        \"id\": root.attrib['id'],\n",
        "        \"writer_id\": root.attrib['writer-id']\n",
        "    }\n",
        "\n",
        "    word_info = []\n",
        "    handwriting_bbox = {\"min_x\": float(\"inf\"), \"min_y\": float(\"inf\"),\n",
        "                        \"max_x\": float(\"-inf\"), \"max_y\": float(\"-inf\")}\n",
        "\n",
        "    for handwritten_part in root.findall(\".//handwritten-part\"):\n",
        "        lines_bboxes = []\n",
        "        for line in handwritten_part.findall(\"line\"):\n",
        "            line_bbox = {\"min_x\": float(\"inf\"), \"min_y\": float(\"inf\"),\n",
        "                    \"max_x\": float(\"-inf\"), \"max_y\": float(\"-inf\")}\n",
        "            for word in line.findall(\"word\"):\n",
        "                word_id = word.attrib['id']\n",
        "                transcription = word.attrib['text']\n",
        "                bboxes = []\n",
        "\n",
        "                coord_blocks = word.findall(\"cmp\")\n",
        "\n",
        "                if len(coord_blocks) > 0:\n",
        "                  for cmp in coord_blocks:\n",
        "                      x, y, width, height = int(cmp.attrib['x']), int(cmp.attrib['y']), int(cmp.attrib['width']), int(cmp.attrib['height'])\n",
        "                      bboxes.append({\"x\": x, \"y\": y, \"width\": width, \"height\": height})\n",
        "\n",
        "                      handwriting_bbox['min_x'] = min(handwriting_bbox['min_x'], x)\n",
        "                      handwriting_bbox['min_y'] = min(handwriting_bbox['min_y'], y)\n",
        "                      handwriting_bbox['max_x'] = max(handwriting_bbox['max_x'], x + width)\n",
        "                      handwriting_bbox['max_y'] = max(handwriting_bbox['max_y'], y + height)\n",
        "\n",
        "                      line_bbox['min_x'] = min(line_bbox['min_x'], x)\n",
        "                      line_bbox['min_y'] = min(line_bbox['min_y'], y)\n",
        "                      line_bbox['max_x'] = max(line_bbox['max_x'], x + width)\n",
        "                      line_bbox['max_y'] = max(line_bbox['max_y'], y + height)\n",
        "\n",
        "                  min_x = min(bbox['x'] for bbox in bboxes)\n",
        "                  min_y = min(bbox['y'] for bbox in bboxes)\n",
        "                  max_x = max(bbox['x'] + bbox['width'] for bbox in bboxes)\n",
        "                  max_y = max(bbox['y'] + bbox['height'] for bbox in bboxes)\n",
        "                  word_bbox = [min_x, min_y, max_x, max_y]\n",
        "\n",
        "                  word_info.append({\n",
        "                      \"word_id\": word_id,\n",
        "                      \"transcription\": transcription,\n",
        "                      \"bbox\": word_bbox\n",
        "                  })\n",
        "                else:\n",
        "                  print(f\"Skipped word: {transcription}, word id: {word_id}, because no coords provided for bbox\")\n",
        "\n",
        "            lines_bboxes.append([line_bbox[\"min_x\"], line_bbox['min_y'], line_bbox['max_x'], line_bbox['max_y'] ])\n",
        "\n",
        "    handwriting_bbox_final = [\n",
        "        handwriting_bbox['min_x'], handwriting_bbox['min_y'],\n",
        "        handwriting_bbox['max_x'], handwriting_bbox['max_y']\n",
        "    ]\n",
        "\n",
        "    result = {\n",
        "        \"form_info\": form_info,\n",
        "        \"words\": word_info,\n",
        "        \"lines_bboxes\": lines_bboxes,\n",
        "        \"handwriting_bbox\": handwriting_bbox_final\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "xml_file = \"/content/xml/a01-000u.xml\"\n",
        "parsed_data = parse_iam_annotation(xml_file)\n",
        "print(parsed_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "kXDivNjSN7Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw_bboxes(image_path, annotations,\n",
        "                output_path=\"/content/annotated_image.jpg\",\n",
        "                draw_handwriting=True):\n",
        "    image = Image.open(image_path)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    if draw_handwriting:\n",
        "      if annotations[\"handwriting_bbox\"]:\n",
        "          bbox = annotations[\"handwriting_bbox\"]\n",
        "          draw.rectangle(bbox, outline=\"red\", width=2)\n",
        "\n",
        "    for line_bbox in annotations[\"lines_bboxes\"]:\n",
        "        if line_bbox[0] < float('inf'):\n",
        "            draw.rectangle(line_bbox, outline=\"green\", width=2)\n",
        "\n",
        "    for word in annotations[\"words\"]:\n",
        "        if word[\"bbox\"]:\n",
        "            bbox = word[\"bbox\"]\n",
        "            draw.rectangle(bbox, outline=\"blue\", width=2)\n",
        "\n",
        "    image.save(output_path)\n",
        "    print(f\"Annotated image saved at {output_path}\")\n",
        "\n",
        "draw_bboxes('/content/data/000/a01-000u.png', parsed_data)\n"
      ],
      "metadata": {
        "id": "a308Xr9rRpOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_from_bbox(image_path, handwriting_bbox, out_path, annotation):\n",
        "  image = Image.open(image_path).crop(handwriting_bbox)\n",
        "  w, h = image.size\n",
        "  for word in annotation['words']:\n",
        "    x_min, y_min, x_max, y_max = word['bbox']\n",
        "    word['bbox'] = [\n",
        "        x_min - handwriting_bbox[0],\n",
        "        y_min - handwriting_bbox[1],\n",
        "        x_max - handwriting_bbox[0],\n",
        "        y_max - handwriting_bbox[1]\n",
        "    ]\n",
        "\n",
        "    if min(word['bbox']) < 0 or word['bbox'][-2] > w or word['bbox'][-1] > h:\n",
        "      raise ValueError(f\"bbox is out of bounds {word['bbox']} size: {image.size}\")\n",
        "  new_lines_bboxes = []\n",
        "  for line_bbox in annotation['lines_bboxes']:\n",
        "    x_min, y_min, x_max, y_max = line_bbox\n",
        "    line_bbox = [\n",
        "        x_min - handwriting_bbox[0],\n",
        "        y_min - handwriting_bbox[1],\n",
        "        x_max - handwriting_bbox[0],\n",
        "        y_max - handwriting_bbox[1]\n",
        "    ]\n",
        "    new_lines_bboxes.append(line_bbox)\n",
        "  annotation['lines_bboxes'] = new_lines_bboxes\n",
        "\n",
        "  if min(line_bbox) < 0 or line_bbox[-2] > w or line_bbox[-1] > h:\n",
        "    raise ValueError(f\"bbox is out of bounds {line_bbox} size: {image.size}\")\n",
        "\n",
        "  image.save(out_path, \"JPEG\")\n",
        "  return annotation"
      ],
      "metadata": {
        "id": "I5OYBZjuSTlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_p = test_files_paths[0]\n",
        "id = os.path.basename(img_p)[:-4]\n",
        "xml_path = os.path.join('/content/xml', id + '.xml')\n",
        "parsed_data = parse_iam_annotation(xml_path)\n",
        "annotation = crop_from_bbox(img_p, parsed_data['handwriting_bbox'],\n",
        "                            '/content/cropping_example.jpg', parsed_data)\n",
        "draw_bboxes('/content/cropping_example.jpg', annotation, draw_handwriting=False)"
      ],
      "metadata": {
        "id": "493JP1D785X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_base_filename(file_path):\n",
        "    base = os.path.basename(file_path)\n",
        "    while base.endswith('.png'):\n",
        "        base = os.path.splitext(base)[0]\n",
        "    return base\n",
        "def crop_save_and_extract(files_paths, out_dir,\n",
        "                          annotation_data,\n",
        "                          annotations_json_path):\n",
        "\n",
        "  for img_p in files_paths:\n",
        "    id = extract_base_filename(img_p)\n",
        "    xml_path = os.path.join('/content/xml', id + '.xml')\n",
        "    parsed_data = parse_iam_annotation(xml_path)\n",
        "\n",
        "    if parsed_data['form_info']['id'] != id:\n",
        "      raise ValueError(\"Id's don't match\")\n",
        "\n",
        "    out_img_path = os.path.join(out_dir, id + '.jpg')\n",
        "\n",
        "    parsed_data = crop_from_bbox(img_p, parsed_data[\"handwriting_bbox\"],\n",
        "                                out_img_path, parsed_data)\n",
        "\n",
        "    annotation_data[id] = {\"writer_id\": parsed_data['form_info']['writer_id'],\n",
        "                           \"words\": parsed_data['words'],\n",
        "                           \"lines_bboxes\": parsed_data['lines_bboxes']}\n",
        "\n",
        "  with open(annotations_json_path, 'w') as f:\n",
        "    json.dump(annotation_data, f)"
      ],
      "metadata": {
        "id": "3ERULmY-F7CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "test_annotation_data = {}\n",
        "train_annotation_data = {}\n",
        "validation_annotation_data = {}\n",
        "\n",
        "crop_save_and_extract(test_files_paths,\n",
        "                      out_test_dir,\n",
        "                      test_annotation_data,\n",
        "                      '/content/SOURCE2/test_annotations.json')\n",
        "\n",
        "crop_save_and_extract(train_files_paths,\n",
        "                      out_train_dir,\n",
        "                      train_annotation_data,\n",
        "                      '/content/SOURCE2/train_annotations.json')\n",
        "\n",
        "crop_save_and_extract(validation_files_paths,\n",
        "                      out_validation_dir,\n",
        "                      validation_annotation_data,\n",
        "                      '/content/SOURCE2/validation_annotations.json')"
      ],
      "metadata": {
        "id": "4VOFC_4cnPKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('/content/detection', \"zip\", '/content/SOURCE2/')"
      ],
      "metadata": {
        "id": "7oL5Cfa8vhGf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}