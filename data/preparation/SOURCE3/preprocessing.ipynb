{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qe0TJfMWxhh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Datasets/SOURCE4/cvl-database-1-1.zip' ,'r') as cvl_f:\n",
        "  cvl_f.extractall('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_TRAIN_DIR = '/content/cvl-database-1-1/trainset/words'\n",
        "WORDS_TRAIN_SUBDIRS = [d for d in os.listdir(WORDS_TRAIN_DIR) if os.path.isdir(os.path.join(WORDS_TRAIN_DIR, d))]\n",
        "\n",
        "WORDS_SUBDIRS_FILES = {}\n",
        "for d in WORDS_TRAIN_SUBDIRS:\n",
        "  WORDS_SUBDIRS_FILES[d] = [f for f in os.listdir(os.path.join(WORDS_TRAIN_DIR, d)) if f[-4:] == \".tif\"]"
      ],
      "metadata": {
        "id": "i7eivzmiYISH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(WORDS_SUBDIRS_FILES)"
      ],
      "metadata": {
        "id": "k5jGUr_-mdFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_w(filename):\n",
        "    pattern = r'\\d+-\\d+-\\d+-\\d+-(.+)\\.tif'\n",
        "\n",
        "    match = re.search(pattern, filename)\n",
        "\n",
        "    if match:\n",
        "        words = match.group(1)\n",
        "        return words\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "hy5jvBMKqkwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_FROM_SUBDIRS = {}\n",
        "for d in [*WORDS_SUBDIRS_FILES]:\n",
        "  WORDS_FROM_SUBDIRS[d] = [extract_w(f) for f in WORDS_SUBDIRS_FILES[d]]"
      ],
      "metadata": {
        "id": "pEDSW_Q_me8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "freq_df = pd.read_csv('/content/drive/MyDrive/Datasets/etc/unigram_freq.csv')\n",
        "freq_df = freq_df.sort_values(by=['count'], axis=0, ascending=False)\n",
        "freq_words = freq_df['word'].to_numpy()\n",
        "freq_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3xuVN6Hboce4",
        "outputId": "d1464902-8987-4077-d7fb-06d54fdb153d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  word        count\n",
              "0  the  23135851162\n",
              "1   of  13151942776\n",
              "2  and  12997637966\n",
              "3   to  12136980858\n",
              "4    a   9081174698"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6269e639-cc00-4a83-91d7-9b2dfb5b8d4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>23135851162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>of</td>\n",
              "      <td>13151942776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and</td>\n",
              "      <td>12997637966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>to</td>\n",
              "      <td>12136980858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a</td>\n",
              "      <td>9081174698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6269e639-cc00-4a83-91d7-9b2dfb5b8d4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6269e639-cc00-4a83-91d7-9b2dfb5b8d4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6269e639-cc00-4a83-91d7-9b2dfb5b8d4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-755a180a-be4a-40ea-8b99-2e1b5c146d18\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-755a180a-be4a-40ea-8b99-2e1b5c146d18')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-755a180a-be4a-40ea-8b99-2e1b5c146d18 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "freq_df"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install english-words lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrmpuV4s0kP-",
        "outputId": "587afa9c-9af2-4bc7-8bf6-ffde2d4ffbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: english-words in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from english_words import get_english_words_set\n",
        "import numpy as np\n",
        "\n",
        "words_list = list(get_english_words_set(['web2']))\n",
        "\n",
        "SOURCE2_WORDS_PATH = '/content/drive/MyDrive/Datasets/SOURCE2/words_new.txt'\n",
        "words_txt = []\n",
        "with open(SOURCE2_WORDS_PATH, 'r') as w_f:\n",
        "    lines = w_f.readlines()\n",
        "    for line in lines:\n",
        "        if line[0] == \"#\":\n",
        "            continue\n",
        "        spl = line.strip().split(\" \")\n",
        "        words_txt.append(spl[-1])\n",
        "\n",
        "unique_source2_words = np.unique(np.array(words_txt))\n",
        "freq_without_source2 = [w for w in freq_words if w not in unique_source2_words]\n",
        "freq_source2_intersection = [w for w in freq_words if w in unique_source2_words]\n"
      ],
      "metadata": {
        "id": "Hr-1Y1oR0b7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vocab_size = 50250\n",
        "\n",
        "freq_new_size = model_vocab_size - len(unique_source2_words)\n",
        "cropped_freq = freq_without_source2[:freq_new_size]\n",
        "\n",
        "unique_source2_words, source2_counts = np.unique(words_txt, return_counts=True)\n",
        "sorted_pairs = sorted(zip(unique_source2_words, source2_counts), key=lambda x: x[1], reverse=True)\n",
        "sorted_unique, sorted_counts = zip(*sorted_pairs)\n",
        "\n",
        "sorted_unique = list(sorted_unique)\n",
        "sorted_counts = list(sorted_counts)\n",
        "\n",
        "total_counts = sum(sorted_counts)\n",
        "probabilities = [count / total_counts for count in sorted_counts]"
      ],
      "metadata": {
        "id": "R2HzFnBO0gaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_IN_DICT = {}\n",
        "WORDS_NOT_IN_DICT = {}\n",
        "\n",
        "for d in [*WORDS_FROM_SUBDIRS]:\n",
        "  WORDS_IN_DICT[d] = [w for w in WORDS_FROM_SUBDIRS[d] if w in cropped_freq or w in sorted_unique]\n",
        "  WORDS_NOT_IN_DICT[d] = [w for w in WORDS_FROM_SUBDIRS[d] if w not in cropped_freq and w not in sorted_unique]"
      ],
      "metadata": {
        "id": "U5UkO_EE0vGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "overall_valid_train = sum([len(WORDS_IN_DICT[d]) for d in [*WORDS_IN_DICT]])\n",
        "valid_dirs_counts = [(d, len(WORDS_IN_DICT[d])) for d in [*WORDS_IN_DICT]]\n",
        "# validation count divided by avg. writer dir valid words len\n",
        "target_size = math.ceil((overall_valid_train * 0.13) / 360.0)\n",
        "VALIDATION_DIRS_TRAINSET = [*WORDS_IN_DICT][:target_size]\n",
        "VAL_WORDS_TRAINSET = {}\n",
        "\n",
        "for dir in VALIDATION_DIRS_TRAINSET:\n",
        "  VAL_WORDS_TRAINSET[dir] = WORDS_IN_DICT[dir]\n",
        "  del WORDS_IN_DICT[dir]\n",
        "\n",
        "print(len([*WORDS_IN_DICT]), len([*WORDS_FROM_SUBDIRS]))\n",
        "print(len([*WORDS_NOT_IN_DICT]), len([*WORDS_FROM_SUBDIRS]))"
      ],
      "metadata": {
        "id": "c3ugg3n03oZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db414b72-bbeb-4bad-8d7e-a51d4d5df254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23 27\n",
            "27 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "out_train_dir = '/content/SOURCE3/train'\n",
        "out_validation_dir = '/content/SOURCE3/validation'\n",
        "out_test_dir = '/content/SOURCE3/test'\n",
        "\n",
        "train_csv = '/content/SOURCE3/train.csv'\n",
        "validation_csv = '/content/SOURCE3/validation.csv'\n",
        "test_csv = '/content/SOURCE3/test.csv'\n",
        "\n",
        "os.makedirs(out_train_dir, exist_ok=True)\n",
        "os.makedirs(out_validation_dir, exist_ok=True)\n",
        "os.makedirs(out_test_dir, exist_ok=True)\n",
        "\n",
        "def generate_filename(frmt='jpg'):\n",
        "    return str(time.time()) + '_' + str(random.randint(100, 999)) + '.' + frmt"
      ],
      "metadata": {
        "id": "tn98pcWx4dzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_TEST_DIR = '/content/cvl-database-1-1/testset/words'\n",
        "WORDS_TESR_SUBDIRS = [d for d in os.listdir(WORDS_TEST_DIR) if os.path.isdir(os.path.join(WORDS_TEST_DIR, d))]\n",
        "\n",
        "TEST_SUBDIRS_FILES = {}\n",
        "for d in WORDS_TESR_SUBDIRS:\n",
        "  TEST_SUBDIRS_FILES[d] = [f for f in os.listdir(os.path.join(WORDS_TEST_DIR, d)) if f[-4:] == \".tif\"]"
      ],
      "metadata": {
        "id": "cg8T91LsGZHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_FROM_TEST_SUBDIRS = {}\n",
        "for d in [*TEST_SUBDIRS_FILES]:\n",
        "  WORDS_FROM_TEST_SUBDIRS[d] = [extract_w(f) for f in TEST_SUBDIRS_FILES[d]]"
      ],
      "metadata": {
        "id": "geeQZH-oHYXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_WORDS_IN_DICT = {}\n",
        "\n",
        "for d in [*WORDS_FROM_TEST_SUBDIRS]:\n",
        "  TEST_WORDS_IN_DICT[d] = [w for w in WORDS_FROM_TEST_SUBDIRS[d] if w in cropped_freq or w in sorted_unique]\n"
      ],
      "metadata": {
        "id": "2Khx7zThM1hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "overall_valid_test = sum([len(TEST_WORDS_IN_DICT[d]) for d in [*TEST_WORDS_IN_DICT]])\n",
        "overall_valid = overall_valid_test + overall_valid_test\n",
        "\n",
        "tagret_test_len = int(overall_valid * 0.2)\n",
        "min_test_len = tagret_test_len - 260\n",
        "max_test_len = tagret_test_len + 260\n",
        "\n",
        "test_dirnames = []\n",
        "train_dirnames = []\n",
        "validation_dirnames =[]\n",
        "\n",
        "cur_test_len = 0\n",
        "achieved_target = False\n",
        "\n",
        "for d in [*TEST_WORDS_IN_DICT]:\n",
        "  if not achieved_target:\n",
        "    cur_test_len += len(TEST_WORDS_IN_DICT[d])\n",
        "    test_dirnames.append(d)\n",
        "\n",
        "    if cur_test_len > min_test_len and cur_test_len < max_test_len:\n",
        "      achieved_target = True\n",
        "    elif cur_test_len > min_test_len and cur_test_len > max_test_len:\n",
        "      raise ValueError(\"Unexpected comparison situation\")\n",
        "\n",
        "  else:\n",
        "    if random.random() <= 0.13:\n",
        "      validation_dirnames.append(d)\n",
        "    else:\n",
        "      train_dirnames.append(d)\n",
        "\n"
      ],
      "metadata": {
        "id": "jZxrpmr-Iqms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cur_test_len/overall_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wnhaqNMJcEY",
        "outputId": "fb43c615-e71f-4e1d-d224-13ccae544291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1990114025726001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "files_writers_ids = []\n",
        "initial_filenames_list = []\n",
        "\n",
        "\n",
        "def process_images(words_dir, dir_names, words_from_subdirs, subdirs_files,\n",
        "                   out_dir, cropped_freq, sorted_unique):\n",
        "    for d in dir_names:\n",
        "        for idx in range(len(words_from_subdirs[d])):\n",
        "            word = words_from_subdirs[d][idx]\n",
        "            if word in cropped_freq or word in sorted_unique:\n",
        "              src_path = os.path.join(words_dir, d, subdirs_files[d][idx])\n",
        "\n",
        "              with Image.open(src_path) as img:\n",
        "                  out_filename = generate_filename()\n",
        "                  out_path = os.path.join(out_dir, out_filename)\n",
        "                  while os.path.exists(out_path):\n",
        "                      out_path = os.path.join(out_dir, generate_filename())\n",
        "\n",
        "                  img.convert('RGB').save(out_path, 'JPEG')\n",
        "\n",
        "                  filenames_list.append(out_filename)\n",
        "                  transcriptions_list.append(word)\n",
        "                  files_writers_ids.append(d)\n",
        "                  initial_filenames_list.append(subdirs_files[d][idx])\n",
        "\n",
        "\n",
        "def save_to_csv_and_archive(filenames_list, transcriptions_list, files_writers_ids,\n",
        "                            initial_filenames_list, csv_path, archive_path, out_dir):\n",
        "    df = pd.DataFrame({\n",
        "        'filename': filenames_list,\n",
        "        'transcription': transcriptions_list,\n",
        "        'writer_id': files_writers_ids,\n",
        "        'initial_filename': initial_filenames_list\n",
        "    })\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    shutil.make_archive(archive_path, \"zip\", out_dir)"
      ],
      "metadata": {
        "id": "0pfxOYkVe_F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_images(\n",
        "    WORDS_TEST_DIR, test_dirnames, WORDS_FROM_TEST_SUBDIRS, TEST_SUBDIRS_FILES, out_test_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "save_to_csv_and_archive(\n",
        "    filenames_list, transcriptions_list, files_writers_ids,\n",
        "    initial_filenames_list, test_csv, \"/content/SOURCE3/test\", out_test_dir)\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "files_writers_ids = []\n",
        "initial_filenames_list = []\n",
        "\n",
        "\n",
        "process_images(\n",
        "    WORDS_TRAIN_DIR, [*WORDS_IN_DICT], WORDS_FROM_SUBDIRS,\n",
        "    WORDS_SUBDIRS_FILES, out_train_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "process_images(\n",
        "    WORDS_TEST_DIR, train_dirnames, WORDS_FROM_TEST_SUBDIRS,\n",
        "    TEST_SUBDIRS_FILES, out_train_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "save_to_csv_and_archive(\n",
        "    filenames_list, transcriptions_list, files_writers_ids,\n",
        "    initial_filenames_list, train_csv, \"/content/SOURCE3/train\", out_train_dir)\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "files_writers_ids = []\n",
        "initial_filenames_list = []\n",
        "\n",
        "process_images(\n",
        "    WORDS_TRAIN_DIR, VALIDATION_DIRS_TRAINSET, WORDS_FROM_SUBDIRS,\n",
        "    WORDS_SUBDIRS_FILES, out_validation_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "process_images(\n",
        "    WORDS_TEST_DIR, validation_dirnames, WORDS_FROM_TEST_SUBDIRS,\n",
        "    TEST_SUBDIRS_FILES, out_train_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "save_to_csv_and_archive(\n",
        "    filenames_list, transcriptions_list, files_writers_ids,\n",
        "    initial_filenames_list, validation_csv, \"/content/SOURCE3/validation\", out_validation_dir)"
      ],
      "metadata": {
        "id": "2Di1_-e5grxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "\n",
        "def extract_bbox_coords(points):\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    if not points:  # If points list is empty, return None\n",
        "        print(\"No points found for bounding box.\")\n",
        "        return None\n",
        "\n",
        "    for point in points:\n",
        "        x = point.get('x')\n",
        "        y = point.get('y')\n",
        "\n",
        "        if x is None or y is None:  # Check if 'x' or 'y' attributes are missing\n",
        "            print(f\"Point missing x or y attribute: {point.attrib}\")\n",
        "            return None\n",
        "\n",
        "        xs.append(float(x))\n",
        "        ys.append(float(y))\n",
        "\n",
        "    return [min(xs), min(ys), max(xs), max(ys)]\n",
        "\n",
        "def extract_annotations(xml_filepath):\n",
        "    ns = {'pc': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2010-03-19'}\n",
        "\n",
        "    with open(xml_filepath, 'rb') as file:\n",
        "        tree = etree.parse(file)\n",
        "\n",
        "    handwriting_crop_bbox = []\n",
        "    page_id = None\n",
        "\n",
        "    for attr_region in tree.findall('.//pc:AttrRegion[@attrType=\"3\"][@fontType=\"2\"]', namespaces=ns):\n",
        "        page_id = attr_region.get('id')\n",
        "        if page_id is None:\n",
        "            print(\"No page ID found.\")\n",
        "            page_id = None\n",
        "\n",
        "        min_area_rect = attr_region.find('.//pc:minAreaRect', namespaces=ns)\n",
        "        if min_area_rect is not None:\n",
        "            handwriting_crop_bbox = extract_bbox_coords(min_area_rect.findall('.//pc:Point', namespaces=ns))\n",
        "        else:\n",
        "            print(f\"No 'minAreaRect' found for page ID: {page_id}\")\n",
        "            handwriting_crop_bbox = None\n",
        "\n",
        "    annotated_regions = []\n",
        "    annotated_regions_contents = []\n",
        "\n",
        "    for region in tree.findall('.//pc:AttrRegion[@attrType=\"2\"][@fontType=\"2\"]', namespaces=ns):\n",
        "        region_id = region.get('id')\n",
        "        if region_id is None:\n",
        "            print(\"No region ID found.\")\n",
        "            region_id = None\n",
        "\n",
        "        median_word_height = region.get('medianWordHeight')\n",
        "        if median_word_height is None:\n",
        "            print(f\"No medianWordHeight found for region ID: {region_id}\")\n",
        "            median_word_height = None\n",
        "        else:\n",
        "            median_word_height = float(median_word_height)\n",
        "\n",
        "        font_angle_rad = region.get('fontAngleRad')\n",
        "        if font_angle_rad is None:\n",
        "            print(f\"No fontAngleRad found for region ID: {region_id}\")\n",
        "            font_angle_rad = None\n",
        "        else:\n",
        "            font_angle_rad = float(font_angle_rad)\n",
        "\n",
        "        min_area_rect = region.find('.//pc:minAreaRect', namespaces=ns)\n",
        "        if min_area_rect is not None:\n",
        "            region_bbox = extract_bbox_coords(min_area_rect.findall('.//pc:Point', namespaces=ns))\n",
        "        else:\n",
        "            print(f\"No 'minAreaRect' found for region ID: {region_id}\")\n",
        "            region_bbox = None\n",
        "\n",
        "        annotated_regions.append({\n",
        "            \"region_id\": region_id,\n",
        "            \"median_word_height\": median_word_height,\n",
        "            \"font_angle_rad\": font_angle_rad,\n",
        "            \"region_bbox\": region_bbox,\n",
        "        })\n",
        "\n",
        "        region_contents = {\n",
        "            \"transcriptions\": [],\n",
        "            \"ids\": [],\n",
        "            \"bboxes\": []\n",
        "        }\n",
        "\n",
        "        # Find all subregions within the region that contain transcriptions\n",
        "        for subregion in region.findall('.//pc:AttrRegion[@text]', namespaces=ns):\n",
        "            text = subregion.get('text')\n",
        "            if text is None:\n",
        "                print(f\"No text found in subregion for region ID: {region_id}\")\n",
        "                text = None\n",
        "\n",
        "            id = subregion.get('id')\n",
        "            if id is None:\n",
        "                print(f\"No ID found for subregion in region ID: {region_id}\")\n",
        "                id = None\n",
        "\n",
        "            min_area_rect = subregion.find('.//pc:minAreaRect', namespaces=ns)\n",
        "            if min_area_rect is not None:\n",
        "                bbox = extract_bbox_coords(min_area_rect.findall('.//pc:Point', namespaces=ns))\n",
        "            else:\n",
        "                print(f\"No 'minAreaRect' found for subregion ID: {id}\")\n",
        "                bbox = None  # Set to None if not found\n",
        "\n",
        "            region_contents[\"transcriptions\"].append(text)\n",
        "            region_contents[\"ids\"].append(id)\n",
        "            region_contents[\"bboxes\"].append(bbox)\n",
        "\n",
        "        annotated_regions_contents.append(region_contents)\n",
        "\n",
        "    return {\n",
        "        \"page_id\": page_id,\n",
        "        \"handwriting_crop_bbox\": handwriting_crop_bbox,\n",
        "        \"regions\": annotated_regions,\n",
        "        \"regions_contents\": annotated_regions_contents,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "V45Vmr15mqZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw_bboxes(image_path, annotations, output_path=\"/content/annotated_image.tif\"):\n",
        "    image = Image.open(image_path)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    if annotations[\"handwriting_crop_bbox\"]:\n",
        "        bbox = annotations[\"handwriting_crop_bbox\"]\n",
        "        draw.rectangle(bbox, outline=\"red\", width=2)\n",
        "\n",
        "    for region in annotations[\"regions\"]:\n",
        "        if region[\"region_bbox\"]:\n",
        "            bbox = region[\"region_bbox\"]\n",
        "            draw.rectangle(bbox, outline=\"blue\", width=2)\n",
        "\n",
        "    for region_contents in annotations[\"regions_contents\"]:\n",
        "        for bbox in region_contents[\"bboxes\"]:\n",
        "            if bbox:\n",
        "                draw.rectangle(bbox, outline=\"green\", width=2)\n",
        "\n",
        "    # Save the image\n",
        "    image.save(output_path)\n",
        "    print(f\"Annotated image saved at {output_path}\")"
      ],
      "metadata": {
        "id": "KGKjE0ndqbqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoding(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    content = content.replace(b'encoding=\"UTF-16\"', b'encoding=\"UTF-8\"')\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(content)\n",
        "\n"
      ],
      "metadata": {
        "id": "QPWo6h9bn4U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_from_bbox(image_path, bbox, out_path):\n",
        "  image = Image.open(image_path).crop(bbox)\n",
        "  image.save(out_path, \"JPEG\", quality=95)"
      ],
      "metadata": {
        "id": "uca7OvQBtJ97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pages_annotations = {}\n",
        "val_pages_annotations = {}\n",
        "test_pages_annotations = {}\n",
        "\n",
        "annotations_dir = '/content/SOURCE3/detection'\n",
        "annotations_train_dir = '/content/SOURCE3/detection/train'\n",
        "annotations_test_dir = '/content/SOURCE3/detection/test'\n",
        "annotations_val_dir = '/content/SOURCE3/detection/validation'\n",
        "\n",
        "os.makedirs(annotations_dir, exist_ok=True)\n",
        "os.makedirs(annotations_train_dir, exist_ok=True)\n",
        "os.makedirs(annotations_test_dir, exist_ok=True)\n",
        "os.makedirs(annotations_val_dir, exist_ok=True)\n",
        "\n",
        "train_xml_dir = '/content/cvl-database-1-1/trainset/xml'\n",
        "test_xml_dir = '/content/cvl-database-1-1/testset/xml'\n",
        "\n",
        "train_pages_dir = '/content/cvl-database-1-1/trainset/pages'\n",
        "test_pages_dir = '/content/cvl-database-1-1/testset/pages'\n",
        "\n",
        "# Throwing out german texts (most of them are corrupted, not relevant)\n",
        "train_xml_files = [f for f in os.listdir(train_xml_dir)\\\n",
        "                   if f[-4:] == '.xml' and '-6_attributes.xml'\\\n",
        "                   not in f and '-3_attributes.xml' not in f]\n",
        "val_xml_files = [f for f in train_xml_files if f[:4] in VALIDATION_DIRS_TRAINSET]\n",
        "train_xml_files = [f for f in train_xml_files if f[:4] not in VALIDATION_DIRS_TRAINSET]\n",
        "\n",
        "# There are two blank pages left by writer, one corrupted\n",
        "merged_xml_files = [f for f in os.listdir(test_xml_dir)\\\n",
        "                   if f[-4:] == '.xml' and '-6_attributes.xml'\\\n",
        "                   not in f and '-3_attributes.xml' not in f \\\n",
        "                    and '0431-3' not in f and '0431-4' not in f \\\n",
        "                    and '0161-4' not in f and '0161-2' not in f\\\n",
        "                    and '0161-3' not in f and '0161-6' not in f]\n",
        "\n",
        "def process_page_and_xml(xml_files_list, xml_dir, pages_dir, annotations_dir, annotations_dict):\n",
        "  for xml_f in xml_files_list:\n",
        "    xml_filename = os.path.join(xml_dir, xml_f)\n",
        "    print(xml_filename)\n",
        "    label_encoding(xml_filename)\n",
        "    annotation = extract_annotations(xml_filename)\n",
        "\n",
        "    crop_bbox = annotation['handwriting_crop_bbox']\n",
        "    page_id = annotation['page_id']\n",
        "\n",
        "    page_filepath = os.path.join(pages_dir, page_id + '.tif')\n",
        "    out_page_filepath = os.path.join(annotations_dir, page_id + '.jpg')\n",
        "\n",
        "    crop_from_bbox(page_filepath, crop_bbox, out_page_filepath)\n",
        "\n",
        "    annotations_dict[page_id] = {\"regions\": annotation[\"regions\"],\n",
        "                                 \"regions_contents\": annotation[\"regions_contents\"]}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CEbDziIquWwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files_from_merged = []\n",
        "val_files_from_merged = []\n",
        "train_files_from_merged = []\n",
        "\n",
        "for xml_f in merged_xml_files:\n",
        "  xml_writer_id = xml_f[:4]\n",
        "\n",
        "  if xml_writer_id in train_dirnames:\n",
        "      train_files_from_merged.append(xml_f)\n",
        "  elif xml_writer_id in test_dirnames:\n",
        "    test_files_from_merged.append(xml_f)\n",
        "  elif xml_writer_id in validation_dirnames:\n",
        "      val_files_from_merged.append(xml_f)\n",
        "  else:\n",
        "    raise ValueError(\"XML file doesn't belong to any valid writer id's\")\n"
      ],
      "metadata": {
        "id": "xYdCgf_s25RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_count = len(test_files_from_merged)\n",
        "train_count = len(train_files_from_merged)\n",
        "val_count = len(val_files_from_merged)"
      ],
      "metadata": {
        "id": "xYg0J8xMbHh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_count / (test_count + train_count + val_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW1AOQSueZpZ",
        "outputId": "7e610756-5493-4cc9-8f81-10d22301d7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3971631205673759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to split pages a bit differenly that in case when we splitted for words."
      ],
      "metadata": {
        "id": "uvVSoRtLbiDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_files_from_merged = []\n",
        "for xml_f in test_files_from_merged:\n",
        "  if random.random() <= 0.5:\n",
        "    if random.random() <= 0.13:\n",
        "      val_files_from_merged.append(xml_f)\n",
        "    else:\n",
        "      train_files_from_merged.append(xml_f)\n",
        "  else:\n",
        "    new_test_files_from_merged.append(xml_f)\n",
        "\n",
        "test_files_from_merged = new_test_files_from_merged"
      ],
      "metadata": {
        "id": "qEJ_FfF5brYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_count = len(test_files_from_merged)\n",
        "train_count = len(train_files_from_merged)\n",
        "val_count = len(val_files_from_merged)\n",
        "print(test_count / (test_count + train_count + val_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAAtNLE-cnQb",
        "outputId": "838f6c7e-bc12-40a2-f412-acff88f7c3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.20212765957446807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aNGb_wHzhn3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "process_page_and_xml(\n",
        "    train_xml_files, train_xml_dir, train_pages_dir,\n",
        "    annotations_train_dir, train_pages_annotations)\n",
        "\n",
        "process_page_and_xml(\n",
        "    train_files_from_merged, test_xml_dir, test_pages_dir,\n",
        "    annotations_train_dir, train_pages_annotations)\n",
        "\n",
        "\n",
        "train_annotations_json = '/content/SOURCE3/detection/train_annotations.json'\n",
        "test_annotations_json = '/content/SOURCE3/detection/test_annotations.json'\n",
        "val_annotations_json = '/content/SOURCE3/detection/validation_annotations.json'\n",
        "\n",
        "\n",
        "with open(train_annotations_json, 'w') as f:\n",
        "  json.dump(train_pages_annotations, f)\n",
        "\n",
        "process_page_and_xml(\n",
        "    test_files_from_merged, test_xml_dir, test_pages_dir,\n",
        "    annotations_test_dir, test_pages_annotations)\n",
        "\n",
        "\n",
        "with open(test_annotations_json, 'w') as f:\n",
        "  json.dump(test_pages_annotations, f)\n",
        "\n",
        "\n",
        "process_page_and_xml(\n",
        "    val_xml_files, train_xml_dir, train_pages_dir,\n",
        "    annotations_val_dir, val_pages_annotations)\n",
        "\n",
        "process_page_and_xml(\n",
        "    val_files_from_merged, test_xml_dir, test_pages_dir,\n",
        "    annotations_val_dir, val_pages_annotations)\n",
        "\n",
        "\n",
        "with open(val_annotations_json, 'w') as f:\n",
        "  json.dump(val_pages_annotations, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "G4VzZi786cT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Percentage of valid from init. testset: {len(merged_xml_files)/len(os.listdir(test_xml_dir)):.2f}')\n",
        "print(f'Percentage of valid from init. train: {len(train_xml_files + val_xml_files)/len(os.listdir(train_xml_dir)):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeQkMmm9_2JQ",
        "outputId": "5a0a9023-8554-4fb4-b9c7-85fba8425424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of valid from init. testset: 0.60\n",
            "Percentage of valid from init. train: 0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('/content/SOURCE3/detection', \"zip\", '/content/SOURCE3/detection/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_nQKE_nOe-jz",
        "outputId": "ba1f194f-e83d-4fe4-b027-044bab989200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/SOURCE3/detection.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLHZ9nWfXfrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}