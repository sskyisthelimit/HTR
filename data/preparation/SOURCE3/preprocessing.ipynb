{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qe0TJfMWxhh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Datasets/SOURCE4/cvl-database-1-1.zip' ,'r') as cvl_f:\n",
        "  cvl_f.extractall('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/Datasets/SOURCE4/test_annotations.json', 'r') as f:\n",
        "  test_annotations = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Datasets/SOURCE4/train_annotations.json', 'r') as f:\n",
        "  train_annotations = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Datasets/SOURCE4/validation_annotations.json', 'r') as f:\n",
        "  validation_annotations = json.load(f)"
      ],
      "metadata": {
        "id": "stybAGpSAunC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_TRAIN_DIR = '/content/cvl-database-1-1/trainset/words'\n",
        "WORDS_TRAIN_SUBDIRS = [d for d in os.listdir(WORDS_TRAIN_DIR) if os.path.isdir(os.path.join(WORDS_TRAIN_DIR, d))]\n",
        "\n",
        "WORDS_SUBDIRS_FILES = {}\n",
        "for d in WORDS_TRAIN_SUBDIRS:\n",
        "  WORDS_SUBDIRS_FILES[d] = [f for f in os.listdir(os.path.join(WORDS_TRAIN_DIR, d)) if f[-4:] == \".tif\"]"
      ],
      "metadata": {
        "id": "i7eivzmiYISH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_w(filename):\n",
        "    pattern = r'\\d+-\\d+-\\d+-\\d+-(.+)\\.tif'\n",
        "\n",
        "    match = re.search(pattern, filename)\n",
        "\n",
        "    if match:\n",
        "        words = match.group(1)\n",
        "        return words\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "hy5jvBMKqkwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_FROM_SUBDIRS = {}\n",
        "for d in [*WORDS_SUBDIRS_FILES]:\n",
        "  WORDS_FROM_SUBDIRS[d] = [extract_w(f) for f in WORDS_SUBDIRS_FILES[d]]"
      ],
      "metadata": {
        "id": "pEDSW_Q_me8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "freq_df = pd.read_csv('/content/drive/MyDrive/Datasets/etc/unigram_freq.csv')\n",
        "freq_df = freq_df.sort_values(by=['count'], axis=0, ascending=False)\n",
        "freq_words = freq_df['word'].to_numpy()\n",
        "freq_df.head()"
      ],
      "metadata": {
        "id": "3xuVN6Hboce4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install english-words lxml"
      ],
      "metadata": {
        "id": "ZrmpuV4s0kP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from english_words import get_english_words_set\n",
        "import numpy as np\n",
        "\n",
        "words_list = list(get_english_words_set(['web2']))\n",
        "\n",
        "SOURCE2_WORDS_PATH = '/content/drive/MyDrive/Datasets/SOURCE2/words_new.txt'\n",
        "words_txt = []\n",
        "with open(SOURCE2_WORDS_PATH, 'r') as w_f:\n",
        "    lines = w_f.readlines()\n",
        "    for line in lines:\n",
        "        if line[0] == \"#\":\n",
        "            continue\n",
        "        spl = line.strip().split(\" \")\n",
        "        words_txt.append(spl[-1])\n",
        "\n",
        "unique_source2_words = np.unique(np.array(words_txt))\n",
        "freq_without_source2 = [w for w in freq_words if w not in unique_source2_words]\n",
        "freq_source2_intersection = [w for w in freq_words if w in unique_source2_words]\n"
      ],
      "metadata": {
        "id": "Hr-1Y1oR0b7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vocab_size = 50250\n",
        "\n",
        "freq_new_size = model_vocab_size - len(unique_source2_words)\n",
        "cropped_freq = freq_without_source2[:freq_new_size]\n",
        "\n",
        "unique_source2_words, source2_counts = np.unique(words_txt, return_counts=True)\n",
        "sorted_pairs = sorted(zip(unique_source2_words, source2_counts), key=lambda x: x[1], reverse=True)\n",
        "sorted_unique, sorted_counts = zip(*sorted_pairs)\n",
        "\n",
        "sorted_unique = list(sorted_unique)\n",
        "sorted_counts = list(sorted_counts)\n",
        "\n",
        "total_counts = sum(sorted_counts)\n",
        "probabilities = [count / total_counts for count in sorted_counts]"
      ],
      "metadata": {
        "id": "R2HzFnBO0gaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_IN_DICT = {}\n",
        "WORDS_NOT_IN_DICT = {}\n",
        "\n",
        "for d in [*WORDS_FROM_SUBDIRS]:\n",
        "  WORDS_IN_DICT[d] = [w for w in WORDS_FROM_SUBDIRS[d] if w in cropped_freq or w in sorted_unique]\n",
        "  WORDS_NOT_IN_DICT[d] = [w for w in WORDS_FROM_SUBDIRS[d] if w not in cropped_freq and w not in sorted_unique]"
      ],
      "metadata": {
        "id": "U5UkO_EE0vGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "VALIDATION_DIRS_TRAINSET = [id.split(\"-\")[0] for id in [*validation_annotations]]\n",
        "VAL_WORDS_TRAINSET = {}\n",
        "\n",
        "for dir in VALIDATION_DIRS_TRAINSET:\n",
        "  if dir in [*WORDS_IN_DICT]:\n",
        "    VAL_WORDS_TRAINSET[dir] = WORDS_IN_DICT[dir]\n",
        "    del WORDS_IN_DICT[dir]\n",
        "  else:\n",
        "    print(f\"Directory {dir} not in WORDS_IN_DICT\")\n",
        "\n",
        "VALIDATION_DIRS_TRAINSET = [*VAL_WORDS_TRAINSET]\n"
      ],
      "metadata": {
        "id": "c3ugg3n03oZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "out_train_dir = '/content/SOURCE3/train'\n",
        "out_validation_dir = '/content/SOURCE3/validation'\n",
        "out_test_dir = '/content/SOURCE3/test'\n",
        "\n",
        "train_csv = '/content/SOURCE3/train.csv'\n",
        "validation_csv = '/content/SOURCE3/validation.csv'\n",
        "test_csv = '/content/SOURCE3/test.csv'\n",
        "\n",
        "os.makedirs(out_train_dir, exist_ok=True)\n",
        "os.makedirs(out_validation_dir, exist_ok=True)\n",
        "os.makedirs(out_test_dir, exist_ok=True)\n",
        "\n",
        "def generate_filename(frmt='jpg'):\n",
        "    return str(time.time()) + '_' + str(random.randint(100, 999)) + '.' + frmt"
      ],
      "metadata": {
        "id": "tn98pcWx4dzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_TEST_DIR = '/content/cvl-database-1-1/testset/words'\n",
        "WORDS_TESR_SUBDIRS = [d for d in os.listdir(WORDS_TEST_DIR) if os.path.isdir(os.path.join(WORDS_TEST_DIR, d))]\n",
        "\n",
        "TEST_SUBDIRS_FILES = {}\n",
        "for d in WORDS_TESR_SUBDIRS:\n",
        "  TEST_SUBDIRS_FILES[d] = [f for f in os.listdir(os.path.join(WORDS_TEST_DIR, d)) if f[-4:] == \".tif\"]"
      ],
      "metadata": {
        "id": "cg8T91LsGZHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS_FROM_TEST_SUBDIRS = {}\n",
        "for d in [*TEST_SUBDIRS_FILES]:\n",
        "  WORDS_FROM_TEST_SUBDIRS[d] = [extract_w(f) for f in TEST_SUBDIRS_FILES[d]]"
      ],
      "metadata": {
        "id": "geeQZH-oHYXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_WORDS_IN_DICT = {}\n",
        "\n",
        "for d in [*WORDS_FROM_TEST_SUBDIRS]:\n",
        "  TEST_WORDS_IN_DICT[d] = [w for w in WORDS_FROM_TEST_SUBDIRS[d] if w in cropped_freq or w in sorted_unique]\n"
      ],
      "metadata": {
        "id": "2Khx7zThM1hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "test_dirnames = []\n",
        "train_dirnames = []\n",
        "validation_dirnames =[]\n",
        "\n",
        "cur_test_len = 0\n",
        "achieved_target = False\n",
        "\n",
        "for d in [*TEST_WORDS_IN_DICT]:\n",
        "  if d in [id.split(\"-\")[0] for id in [*test_annotations]]:\n",
        "    test_dirnames.append(d)\n",
        "  elif d in [id.split(\"-\")[0] for id in [*train_annotations]]:\n",
        "    train_dirnames.append(d)\n",
        "  elif d in [id.split(\"-\")[0] for id in [*validation_annotations]]:\n",
        "    validation_dirnames.append(d)\n",
        "\n",
        "print(len([*TEST_WORDS_IN_DICT]), len(test_dirnames) +\n",
        "      len(train_dirnames) + len(validation_dirnames))"
      ],
      "metadata": {
        "id": "jZxrpmr-Iqms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "files_writers_ids = []\n",
        "initial_filenames_list = []\n",
        "\n",
        "\n",
        "def process_images(words_dir, dir_names, words_from_subdirs, subdirs_files,\n",
        "                   out_dir, cropped_freq, sorted_unique):\n",
        "    for d in dir_names:\n",
        "        for idx in range(len(words_from_subdirs[d])):\n",
        "            word = words_from_subdirs[d][idx]\n",
        "            if word in cropped_freq or word in sorted_unique:\n",
        "              src_path = os.path.join(words_dir, d, subdirs_files[d][idx])\n",
        "\n",
        "              with Image.open(src_path) as img:\n",
        "                  out_filename = generate_filename()\n",
        "                  out_path = os.path.join(out_dir, out_filename)\n",
        "                  while os.path.exists(out_path):\n",
        "                      out_path = os.path.join(out_dir, generate_filename())\n",
        "\n",
        "                  img.convert('RGB').save(out_path, 'JPEG')\n",
        "\n",
        "                  filenames_list.append(out_filename)\n",
        "                  transcriptions_list.append(word)\n",
        "                  files_writers_ids.append(d)\n",
        "                  initial_filenames_list.append(subdirs_files[d][idx])\n",
        "\n",
        "\n",
        "def save_to_csv_and_archive(filenames_list, transcriptions_list, files_writers_ids,\n",
        "                            initial_filenames_list, csv_path, archive_path, out_dir):\n",
        "    df = pd.DataFrame({\n",
        "        'filename': filenames_list,\n",
        "        'transcription': transcriptions_list,\n",
        "        'writer_id': files_writers_ids,\n",
        "        'initial_filename': initial_filenames_list\n",
        "    })\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    shutil.make_archive(archive_path, \"zip\", out_dir)"
      ],
      "metadata": {
        "id": "0pfxOYkVe_F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_images(\n",
        "    WORDS_TEST_DIR, test_dirnames, WORDS_FROM_TEST_SUBDIRS, TEST_SUBDIRS_FILES, out_test_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "save_to_csv_and_archive(\n",
        "    filenames_list, transcriptions_list, files_writers_ids,\n",
        "    initial_filenames_list, test_csv, \"/content/SOURCE3/test\", out_test_dir)\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "files_writers_ids = []\n",
        "initial_filenames_list = []\n",
        "\n",
        "\n",
        "process_images(\n",
        "    WORDS_TRAIN_DIR, [*WORDS_IN_DICT], WORDS_FROM_SUBDIRS,\n",
        "    WORDS_SUBDIRS_FILES, out_train_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "process_images(\n",
        "    WORDS_TEST_DIR, train_dirnames, WORDS_FROM_TEST_SUBDIRS,\n",
        "    TEST_SUBDIRS_FILES, out_train_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "save_to_csv_and_archive(\n",
        "    filenames_list, transcriptions_list, files_writers_ids,\n",
        "    initial_filenames_list, train_csv, \"/content/SOURCE3/train\", out_train_dir)\n",
        "\n",
        "filenames_list = []\n",
        "transcriptions_list = []\n",
        "files_writers_ids = []\n",
        "initial_filenames_list = []\n",
        "\n",
        "process_images(\n",
        "    WORDS_TRAIN_DIR, VALIDATION_DIRS_TRAINSET, WORDS_FROM_SUBDIRS,\n",
        "    WORDS_SUBDIRS_FILES, out_validation_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "process_images(\n",
        "    WORDS_TEST_DIR, validation_dirnames, WORDS_FROM_TEST_SUBDIRS,\n",
        "    TEST_SUBDIRS_FILES, out_validation_dir, cropped_freq, sorted_unique\n",
        ")\n",
        "\n",
        "save_to_csv_and_archive(\n",
        "    filenames_list, transcriptions_list, files_writers_ids,\n",
        "    initial_filenames_list, validation_csv, \"/content/SOURCE3/validation\", out_validation_dir)"
      ],
      "metadata": {
        "id": "2Di1_-e5grxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "\n",
        "def extract_bbox_coords(points):\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    if not points:  # If points list is empty, return None\n",
        "        print(\"No points found for bounding box.\")\n",
        "        return None\n",
        "\n",
        "    for point in points:\n",
        "        x = point.get('x')\n",
        "        y = point.get('y')\n",
        "\n",
        "        if x is None or y is None:  # Check if 'x' or 'y' attributes are missing\n",
        "            print(f\"Point missing x or y attribute: {point.attrib}\")\n",
        "            return None\n",
        "\n",
        "        xs.append(float(x))\n",
        "        ys.append(float(y))\n",
        "\n",
        "    return [min(xs), min(ys), max(xs), max(ys)]\n",
        "\n",
        "def extract_annotations(xml_filepath):\n",
        "    ns = {'pc': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2010-03-19'}\n",
        "\n",
        "    with open(xml_filepath, 'rb') as file:\n",
        "        tree = etree.parse(file)\n",
        "\n",
        "    handwriting_crop_bbox = []\n",
        "    page_id = None\n",
        "\n",
        "    for attr_region in tree.findall('.//pc:AttrRegion[@attrType=\"3\"][@fontType=\"2\"]', namespaces=ns):\n",
        "        page_id = attr_region.get('id')\n",
        "        if page_id is None:\n",
        "            print(\"No page ID found.\")\n",
        "            page_id = None\n",
        "\n",
        "        min_area_rect = attr_region.find('.//pc:minAreaRect', namespaces=ns)\n",
        "        if min_area_rect is not None:\n",
        "            handwriting_crop_bbox = extract_bbox_coords(min_area_rect.findall('.//pc:Point', namespaces=ns))\n",
        "        else:\n",
        "            print(f\"No 'minAreaRect' found for page ID: {page_id}\")\n",
        "            handwriting_crop_bbox = None\n",
        "\n",
        "    annotated_regions = []\n",
        "    annotated_regions_contents = []\n",
        "\n",
        "    for region in tree.findall('.//pc:AttrRegion[@attrType=\"2\"][@fontType=\"2\"]', namespaces=ns):\n",
        "        region_id = region.get('id')\n",
        "        if region_id is None:\n",
        "            print(\"No region ID found.\")\n",
        "            region_id = None\n",
        "\n",
        "        median_word_height = region.get('medianWordHeight')\n",
        "        if median_word_height is None:\n",
        "            print(f\"No medianWordHeight found for region ID: {region_id}\")\n",
        "            median_word_height = None\n",
        "        else:\n",
        "            median_word_height = float(median_word_height)\n",
        "\n",
        "        font_angle_rad = region.get('fontAngleRad')\n",
        "        if font_angle_rad is None:\n",
        "            print(f\"No fontAngleRad found for region ID: {region_id}\")\n",
        "            font_angle_rad = None\n",
        "        else:\n",
        "            font_angle_rad = float(font_angle_rad)\n",
        "\n",
        "        min_area_rect = region.find('.//pc:minAreaRect', namespaces=ns)\n",
        "        if min_area_rect is not None:\n",
        "            region_bbox = extract_bbox_coords(min_area_rect.findall('.//pc:Point', namespaces=ns))\n",
        "        else:\n",
        "            print(f\"No 'minAreaRect' found for region ID: {region_id}\")\n",
        "            region_bbox = None\n",
        "\n",
        "        annotated_regions.append({\n",
        "            \"region_id\": region_id,\n",
        "            \"median_word_height\": median_word_height,\n",
        "            \"font_angle_rad\": font_angle_rad,\n",
        "            \"region_bbox\": region_bbox,\n",
        "        })\n",
        "\n",
        "        region_contents = {\n",
        "            \"transcriptions\": [],\n",
        "            \"ids\": [],\n",
        "            \"bboxes\": []\n",
        "        }\n",
        "\n",
        "        # Find all subregions within the region that contain transcriptions\n",
        "        for subregion in region.findall('.//pc:AttrRegion[@text]', namespaces=ns):\n",
        "            text = subregion.get('text')\n",
        "            if text is None:\n",
        "                print(f\"No text found in subregion for region ID: {region_id}\")\n",
        "                text = None\n",
        "\n",
        "            id = subregion.get('id')\n",
        "            if id is None:\n",
        "                print(f\"No ID found for subregion in region ID: {region_id}\")\n",
        "                id = None\n",
        "\n",
        "            min_area_rect = subregion.find('.//pc:minAreaRect', namespaces=ns)\n",
        "            if min_area_rect is not None:\n",
        "                bbox = extract_bbox_coords(min_area_rect.findall('.//pc:Point', namespaces=ns))\n",
        "            else:\n",
        "                print(f\"No 'minAreaRect' found for subregion ID: {id}\")\n",
        "                bbox = None  # Set to None if not found\n",
        "\n",
        "            region_contents[\"transcriptions\"].append(text)\n",
        "            region_contents[\"ids\"].append(id)\n",
        "            region_contents[\"bboxes\"].append(bbox)\n",
        "\n",
        "        annotated_regions_contents.append(region_contents)\n",
        "\n",
        "    return {\n",
        "        \"page_id\": page_id,\n",
        "        \"handwriting_crop_bbox\": handwriting_crop_bbox,\n",
        "        \"regions\": annotated_regions,\n",
        "        \"regions_contents\": annotated_regions_contents,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "V45Vmr15mqZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw_bboxes(image_path, annotations, output_path=\"/content/annotated_image.tif\"):\n",
        "    image = Image.open(image_path)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    if annotations[\"handwriting_crop_bbox\"]:\n",
        "        bbox = annotations[\"handwriting_crop_bbox\"]\n",
        "        draw.rectangle(bbox, outline=\"red\", width=2)\n",
        "\n",
        "    for region in annotations[\"regions\"]:\n",
        "        if region[\"region_bbox\"]:\n",
        "            bbox = region[\"region_bbox\"]\n",
        "            draw.rectangle(bbox, outline=\"blue\", width=2)\n",
        "\n",
        "    for region_contents in annotations[\"regions_contents\"]:\n",
        "        for bbox in region_contents[\"bboxes\"]:\n",
        "            if bbox:\n",
        "                draw.rectangle(bbox, outline=\"green\", width=2)\n",
        "\n",
        "    # Save the image\n",
        "    image.save(output_path)\n",
        "    print(f\"Annotated image saved at {output_path}\")"
      ],
      "metadata": {
        "id": "KGKjE0ndqbqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoding(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    content = content.replace(b'encoding=\"UTF-16\"', b'encoding=\"UTF-8\"')\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(content)\n",
        "\n"
      ],
      "metadata": {
        "id": "QPWo6h9bn4U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_from_bbox(image_path, bbox, out_path, annotation):\n",
        "  image = Image.open(image_path).crop(bbox)\n",
        "  for region in annotation['regions']:\n",
        "    x_min, y_min, x_max, y_max = region[\"region_bbox\"]\n",
        "    region[\"region_bbox\"] = [x_min - bbox[0], y_min - bbox[1], x_max - bbox[0], y_max - bbox[1]]\n",
        "    w, h = image.size\n",
        "    if (min(region[\"region_bbox\"]) < 0 or\n",
        "        region[\"region_bbox\"][-2] > w or\n",
        "        region[\"region_bbox\"][-1] > h):\n",
        "        raise ValueError(f'bbox is out of bounds {region[\"region_bbox\"]} for image size: {image.size}')\n",
        "\n",
        "  for content in annotation['regions_contents']:\n",
        "    for idx in range(len(content[\"bboxes\"])):\n",
        "      x_min, y_min, x_max, y_max = content[\"bboxes\"][idx]\n",
        "      content[\"bboxes\"][idx] = [x_min - bbox[0], y_min - bbox[1], x_max - bbox[0], y_max - bbox[1]]\n",
        "\n",
        "  image.save(out_path, \"JPEG\", quality=95)\n",
        "  return annotation"
      ],
      "metadata": {
        "id": "uca7OvQBtJ97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pages_annotations = {}\n",
        "val_pages_annotations = {}\n",
        "test_pages_annotations = {}\n",
        "\n",
        "annotations_dir = '/content/SOURCE3/detection'\n",
        "annotations_train_dir = '/content/SOURCE3/detection/train'\n",
        "annotations_test_dir = '/content/SOURCE3/detection/test'\n",
        "annotations_val_dir = '/content/SOURCE3/detection/validation'\n",
        "\n",
        "os.makedirs(annotations_dir, exist_ok=True)\n",
        "os.makedirs(annotations_train_dir, exist_ok=True)\n",
        "os.makedirs(annotations_test_dir, exist_ok=True)\n",
        "os.makedirs(annotations_val_dir, exist_ok=True)\n",
        "\n",
        "train_xml_dir = '/content/cvl-database-1-1/trainset/xml'\n",
        "test_xml_dir = '/content/cvl-database-1-1/testset/xml'\n",
        "\n",
        "train_pages_dir = '/content/cvl-database-1-1/trainset/pages'\n",
        "test_pages_dir = '/content/cvl-database-1-1/testset/pages'\n",
        "\n",
        "# Throwing out german texts (most of them are corrupted, not relevant)\n",
        "train_xml_files = [f for f in os.listdir(train_xml_dir)\\\n",
        "                   if f[-4:] == '.xml' and '-6_attributes.xml'\\\n",
        "                   not in f and '-3_attributes.xml' not in f]\n",
        "val_xml_files = [f for f in train_xml_files if f[:4] in VALIDATION_DIRS_TRAINSET]\n",
        "train_xml_files = [f for f in train_xml_files if f[:4] not in VALIDATION_DIRS_TRAINSET]\n",
        "\n",
        "# There are two blank pages left by writer, one corrupted\n",
        "merged_xml_files = [f for f in os.listdir(test_xml_dir)\\\n",
        "                   if f[-4:] == '.xml' and '-6_attributes.xml'\\\n",
        "                   not in f and '-3_attributes.xml' not in f \\\n",
        "                    and '0431-3' not in f and '0431-4' not in f \\\n",
        "                    and '0161-4' not in f and '0161-2' not in f\\\n",
        "                    and '0161-3' not in f and '0161-6' not in f]\n",
        "\n",
        "def process_page_and_xml(xml_files_list, xml_dir, pages_dir, annotations_dir, annotations_dict):\n",
        "  for xml_f in xml_files_list:\n",
        "    xml_filename = os.path.join(xml_dir, xml_f)\n",
        "    print(xml_filename)\n",
        "    label_encoding(xml_filename)\n",
        "    annotation = extract_annotations(xml_filename)\n",
        "\n",
        "    crop_bbox = annotation['handwriting_crop_bbox']\n",
        "    page_id = annotation['page_id']\n",
        "\n",
        "    page_filepath = os.path.join(pages_dir, page_id + '.tif')\n",
        "    out_page_filepath = os.path.join(annotations_dir, page_id + '.jpg')\n",
        "\n",
        "    annotation = crop_from_bbox(page_filepath, crop_bbox, out_page_filepath, annotation)\n",
        "\n",
        "    annotations_dict[page_id] = {\"regions\": annotation[\"regions\"],\n",
        "                                 \"regions_contents\": annotation[\"regions_contents\"]}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CEbDziIquWwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files_from_merged = []\n",
        "val_files_from_merged = []\n",
        "train_files_from_merged = []\n",
        "\n",
        "for xml_f in merged_xml_files:\n",
        "  xml_writer_id = xml_f[:4]\n",
        "\n",
        "  if xml_writer_id in train_dirnames:\n",
        "      train_files_from_merged.append(xml_f)\n",
        "  elif xml_writer_id in test_dirnames:\n",
        "    test_files_from_merged.append(xml_f)\n",
        "  elif xml_writer_id in validation_dirnames:\n",
        "      val_files_from_merged.append(xml_f)\n",
        "  else:\n",
        "    raise ValueError(\"XML file doesn't belong to any valid writer id's\")\n"
      ],
      "metadata": {
        "id": "xYdCgf_s25RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_count = len(test_files_from_merged)\n",
        "train_count = len(train_files_from_merged)\n",
        "val_count = len(val_files_from_merged)"
      ],
      "metadata": {
        "id": "xYg0J8xMbHh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_count / (test_count + train_count + val_count))"
      ],
      "metadata": {
        "id": "WW1AOQSueZpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to split pages a bit differenly that in case when we splitted for words."
      ],
      "metadata": {
        "id": "uvVSoRtLbiDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_files_from_merged = []\n",
        "for xml_f in test_files_from_merged:\n",
        "  if random.random() <= 0.5:\n",
        "    if random.random() <= 0.13:\n",
        "      val_files_from_merged.append(xml_f)\n",
        "    else:\n",
        "      train_files_from_merged.append(xml_f)\n",
        "  else:\n",
        "    new_test_files_from_merged.append(xml_f)\n",
        "\n",
        "test_files_from_merged = new_test_files_from_merged"
      ],
      "metadata": {
        "id": "qEJ_FfF5brYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_count = len(test_files_from_merged)\n",
        "train_count = len(train_files_from_merged)\n",
        "val_count = len(val_files_from_merged)\n",
        "print(test_count / (test_count + train_count + val_count))"
      ],
      "metadata": {
        "id": "QAAtNLE-cnQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aNGb_wHzhn3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "process_page_and_xml(\n",
        "    train_xml_files, train_xml_dir, train_pages_dir,\n",
        "    annotations_train_dir, train_pages_annotations)\n",
        "\n",
        "process_page_and_xml(\n",
        "    train_files_from_merged, test_xml_dir, test_pages_dir,\n",
        "    annotations_train_dir, train_pages_annotations)\n",
        "\n",
        "\n",
        "train_annotations_json = '/content/SOURCE3/detection/train_annotations.json'\n",
        "test_annotations_json = '/content/SOURCE3/detection/test_annotations.json'\n",
        "val_annotations_json = '/content/SOURCE3/detection/validation_annotations.json'\n",
        "\n",
        "\n",
        "with open(train_annotations_json, 'w') as f:\n",
        "  json.dump(train_pages_annotations, f)\n",
        "\n",
        "process_page_and_xml(\n",
        "    test_files_from_merged, test_xml_dir, test_pages_dir,\n",
        "    annotations_test_dir, test_pages_annotations)\n",
        "\n",
        "\n",
        "with open(test_annotations_json, 'w') as f:\n",
        "  json.dump(test_pages_annotations, f)\n",
        "\n",
        "\n",
        "process_page_and_xml(\n",
        "    val_xml_files, train_xml_dir, train_pages_dir,\n",
        "    annotations_val_dir, val_pages_annotations)\n",
        "\n",
        "process_page_and_xml(\n",
        "    val_files_from_merged, test_xml_dir, test_pages_dir,\n",
        "    annotations_val_dir, val_pages_annotations)\n",
        "\n",
        "\n",
        "with open(val_annotations_json, 'w') as f:\n",
        "  json.dump(val_pages_annotations, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "G4VzZi786cT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Percentage of valid from init. testset: {len(merged_xml_files)/len(os.listdir(test_xml_dir)):.2f}')\n",
        "print(f'Percentage of valid from init. train: {len(train_xml_files + val_xml_files)/len(os.listdir(train_xml_dir)):.2f}')"
      ],
      "metadata": {
        "id": "DeQkMmm9_2JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('/content/SOURCE3/detection', \"zip\", '/content/SOURCE3/detection/')"
      ],
      "metadata": {
        "id": "_nQKE_nOe-jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLHZ9nWfXfrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}