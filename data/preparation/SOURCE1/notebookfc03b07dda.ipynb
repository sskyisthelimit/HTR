{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2243895,"sourceType":"datasetVersion","datasetId":1347338},{"sourceId":9441028,"sourceType":"datasetVersion","datasetId":5737201}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tqdm lightning","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:31:46.954841Z","iopub.execute_input":"2024-09-21T09:31:46.955621Z","iopub.status.idle":"2024-09-21T09:32:02.423184Z","shell.execute_reply.started":"2024-09-21T09:31:46.955558Z","shell.execute_reply":"2024-09-21T09:32:02.422269Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nCollecting lightning\n  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.6)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.4.0)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.4.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.4.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\nDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Optional, Tuple\n\n\nclass DTrOCRConfig:\n    def __init__(\n        self,\n        vit_hf_model: str = 'google/vit-base-patch16-224',\n        t5_model: str = 't5-small',\n        vocab_size: Optional[int] = 32128,\n        max_position_embeddings: Optional[int] = 256,\n        hidden_size: Optional[int] = 512,\n        num_hidden_layers: Optional[int] = 12,\n        feed_forward_proj: str = \"gated-gelu\",\n        num_attention_heads: Optional[int] = 12,\n        patch_size: Tuple[int] = (4, 8),  # (h, w)\n        image_size: Tuple[int] = (32, 128),  # (h, w)\n        num_channels: Optional[int] = 3,\n        resid_pdrop: Optional[float] = 0.1,\n        embd_pdrop: Optional[float] = 0.1,\n        attn_pdrop: Optional[float] = 0.1,\n        layer_norm_epsilon: Optional[float] = 1e-5,\n        attn_implementation: str = 'flash_attention_2'\n    ):\n\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.num_channels = num_channels\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self._attn_implementation = attn_implementation\n        self.vit_hf_model = vit_hf_model\n        # T5 config values\n        self.t5_model = t5_model\n        self.feed_forward_proj = feed_forward_proj\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:02.425061Z","iopub.execute_input":"2024-09-21T09:32:02.425373Z","iopub.status.idle":"2024-09-21T09:32:02.435641Z","shell.execute_reply.started":"2024-09-21T09:32:02.425341Z","shell.execute_reply":"2024-09-21T09:32:02.434582Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Union, List\n\n\n@dataclass\nclass DTrOCRModelOutput:\n    hidden_states: torch.FloatTensor\n    past_key_values: torch.FloatTensor\n\n\n@dataclass\nclass DTrOCRLMHeadModelOutput:\n    logits: torch.FloatTensor\n    loss: Optional[torch.FloatTensor] = None\n    accuracy: Optional[torch.FloatTensor] = None\n    past_key_values: Optional[torch.FloatTensor] = None\n\n\n@dataclass\nclass DTrOCRProcessorOutput:\n    pixel_values: Optional[torch.FloatTensor] = None\n    input_ids: Optional[Union[torch.LongTensor, np.ndarray, List[int]]] = None\n    attention_mask: Optional[Union[torch.FloatTensor, np.ndarray, List[int]]] = None\n    labels: Optional[Union[torch.LongTensor, np.ndarray, List[int]]] = None\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:02.436823Z","iopub.execute_input":"2024-09-21T09:32:02.437091Z","iopub.status.idle":"2024-09-21T09:32:05.350701Z","shell.execute_reply.started":"2024-09-21T09:32:02.437062Z","shell.execute_reply":"2024-09-21T09:32:05.349893Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import T5TokenizerFast, AutoImageProcessor\n\nfrom PIL import Image\nfrom typing import List, Union\n\nclass DTrOCRProcessor:\n    def __init__(self, config: DTrOCRConfig, add_bos_token: bool = False,\n                 add_eos_token: bool = False):\n        self.vit_processor = AutoImageProcessor.from_pretrained(\n            config.vit_hf_model,\n            size={\n                \"height\": config.image_size[0],\n                'width': config.image_size[1]\n            },\n            use_fast=True\n        )\n\n        self.tokenizer = T5TokenizerFast.from_pretrained(\n            config.t5_model,\n            model_max_length=config.max_position_embeddings - int(\n                (config.image_size[0] / config.patch_size[0]) *\n                (config.image_size[1] / config.patch_size[1])\n            )\n        )\n\n        self.add_bos_token = add_bos_token\n        self.add_eos_token = add_eos_token\n\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def __call__(\n        self,\n        images: Union[Image.Image, List[Image.Image]] = None,\n        texts: Union[str, List[str]] = None,\n        return_labels: bool = False,\n        input_data_format: str = 'channels_last',\n        padding: Union[bool, str] = False,\n        *args,\n        **kwargs\n    ) -> DTrOCRProcessorOutput:\n        text_inputs = self.tokenizer(\n            texts, padding=padding, add_special_tokens=True,\n            *args, **kwargs\n        ) if texts is not None else None\n\n        image_inputs = self.vit_processor(\n            images, input_data_format=input_data_format,\n            *args, **kwargs\n        ) if images is not None else None\n\n        return DTrOCRProcessorOutput(\n            pixel_values=image_inputs[\"pixel_values\"] \n            if images is not None else None,\n            \n            input_ids=text_inputs['input_ids']\n            if texts is not None else None,\n            \n            attention_mask=text_inputs['attention_mask']\n            if texts is not None else None,\n            \n            labels=text_inputs['input_ids']\n            if texts is not None and return_labels else None\n        )\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:05.352728Z","iopub.execute_input":"2024-09-21T09:32:05.353108Z","iopub.status.idle":"2024-09-21T09:32:19.005391Z","shell.execute_reply.started":"2024-09-21T09:32:05.353075Z","shell.execute_reply":"2024-09-21T09:32:19.004413Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, Tensor\nimport pytorch_lightning as pl\nfrom typing import Optional, Tuple, Dict, Any\n\nfrom transformers.models.vit.modeling_vit import ViTPatchEmbeddings\nfrom transformers.generation.logits_process import LogitsProcessorList\nfrom transformers import T5EncoderModel\nfrom transformers.generation.configuration_utils import GenerationConfig\nfrom transformers.generation.beam_search import BeamScorer, BeamSearchScorer\nfrom transformers.generation.stopping_criteria import (\n    EosTokenCriteria,\n    MaxLengthCriteria,\n    MaxTimeCriteria,\n    StoppingCriteriaList,\n    StopStringCriteria,\n)\n\n\nclass DTrOCRModel(pl.LightningModule):\n    def __init__(self, config: DTrOCRConfig):\n        super().__init__()\n        # embeddings\n        self.patch_embeddings = ViTPatchEmbeddings(config)\n        self.token_embedding = nn.Embedding(config.vocab_size,\n                                            config.hidden_size)\n        self.positional_embedding = nn.Embedding(\n            config.max_position_embeddings, config.hidden_size)\n\n        self.t5_encoder = T5EncoderModel.from_pretrained(\n            config.t5_model).encoder\n        self.dropout = nn.Dropout(config.attn_pdrop)\n        self.layer_norm = nn.LayerNorm(config.hidden_size,\n                                       eps=config.layer_norm_epsilon)\n\n        self._attn_implementation = config._attn_implementation\n\n    def forward(\n        self,\n        pixel_values: torch.Tensor,  # (batch_size, num_channels, height, width)\n        input_ids: torch.LongTensor,  # (batch_size, seq_len)\n        position_ids: Optional[torch.LongTensor] = None,  # (batch_size, seq_len)\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,  # (batch_size, num_heads, seq_len, hidden_size)\n        attention_mask: Optional[torch.Tensor] = None,  # (batch_size, seq_len)\n        use_cache: Optional[bool] = False,\n    ) -> DTrOCRModelOutput:\n\n\n        input_ids = input_ids.view(-1, input_ids.shape[-1])\n\n        # Past key values initialization\n        if past_key_values is None:\n            past_length = 0\n            past_key_values = tuple([None] * self.t5_encoder.config.num_layers)\n        else:\n            past_length = past_key_values[0][0].size(-2)\n\n        # Get patch embeddings\n        patch_embeddings = self.patch_embeddings(pixel_values)\\\n            if past_length == 0 else None\n        \n        token_embeddings = self.token_embedding(input_ids)\n\n        # Concatenate patch and token embeddings\n        if patch_embeddings is not None:\n            patch_and_token_embeddings = torch.cat(\n                [patch_embeddings, token_embeddings], dim=-2)\n        else:\n            patch_and_token_embeddings = token_embeddings\n\n        input_shape = patch_and_token_embeddings.shape\n\n        # Position embeddings\n        if position_ids is None or past_length == 0:\n            position_ids = torch.arange(\n                past_length, input_shape[1] + past_length,\n                dtype=torch.long)\n            \n            position_ids = position_ids.unsqueeze(0)\n        else:\n            position_ids = torch.ones_like(\n                position_ids,\n                ) * past_length\n        \n        position_embeddings = self.positional_embedding(position_ids)\n\n        # Add patch/token embeddings with positional embeddings\n        hidden_states = patch_and_token_embeddings + position_embeddings\n        hidden_states = self.dropout(hidden_states)\n\n        # Attention mask\n        if attention_mask is not None:\n            attention_mask = torch.cat(\n                [\n                    torch.ones(\n                        attention_mask.shape[0],\n\n                        patch_embeddings.shape[-2] if patch_embeddings\n                        is not None else past_length,\n                        \n                        dtype=attention_mask.dtype,\n                    ),\n                    attention_mask\n                ], dim=-1\n            )\n            if self._attn_implementation == \"flash_attention_2\":\n                attention_mask = attention_mask if 0 in attention_mask else None\n            else:\n                raise ValueError('Incorrect _attn_implementation,'\n                                 'supported only \"flash_attention_2\"')\n\n        # Forward pass through T5 encoder layers\n        encoder_outputs = self.t5_encoder(\n            inputs_embeds=hidden_states,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n        )\n        hidden_states = encoder_outputs.last_hidden_state\n        presents = encoder_outputs.past_key_values\n\n        # Apply final layer normalization\n        hidden_states = self.layer_norm(hidden_states)\n\n        return DTrOCRModelOutput(hidden_states=hidden_states,\n                                 past_key_values=presents)\n\n\nclass DTrOCRLMHeadModel(pl.LightningModule):\n    def __init__(self, config: DTrOCRConfig):\n        super().__init__()\n        self.config = config\n\n        self.transformer = DTrOCRModel(config)\n        self.language_model_head = nn.Linear(config.hidden_size,\n                                             config.vocab_size, bias=False)\n\n        image_size, patch_size = config.image_size, config.patch_size\n        self.image_embedding_length = int(\n            (image_size[0] / patch_size[0]) * (image_size[1] / patch_size[1]))\n\n    def training_step(self, inputs):\n        outputs = self.forward(**inputs)\n        loss = outputs.loss\n\n        self.log_dict(\n            {\n                \"train_loss\": loss,\n            },\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n        \n        return {\"loss\": loss, \"scores\": outputs.accuracy, \"y\": outputs.logits}\n    \n    def validation_step(self, inputs):\n        outputs = self.forward(**inputs)\n        loss = outputs.loss\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, inputs):\n        outputs = self.forward(**inputs)\n        loss = outputs.loss\n        self.log(\"test_loss\", loss)\n        return {\"loss\": loss, \"y\": outputs.logits}\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(params=self.parameters(), lr=1e-4)\n\n    def forward(\n        self,\n        pixel_values: torch.Tensor,\n        input_ids: torch.LongTensor,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = False,\n        labels: Optional[torch.LongTensor] = None,\n    ) -> DTrOCRLMHeadModelOutput:\n        transformer_output = self.transformer(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            past_key_values=past_key_values,\n            position_ids=position_ids,\n            attention_mask=attention_mask,\n            use_cache=use_cache\n        )\n        logits = self.language_model_head(transformer_output.hidden_states)\n\n        loss, accuracy = None, None\n        if labels is not None:\n            labels = labels\n\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., self.image_embedding_length:-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n\n            loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n            loss = loss_fct(shift_logits.view(\n                -1, shift_logits.size(-1)), shift_labels.view(-1))\n\n            label_matches = shift_labels.view(-1) == torch.argmax(\n                torch.nn.functional.softmax(shift_logits.view(\n                    -1, shift_logits.size(-1)), dim=-1), dim=-1\n            )\n\n            # reduce loss\n            if attention_mask is not None:\n                mask = attention_mask[..., 1:].reshape(-1)\n\n                loss = (mask * loss).sum() / mask.sum()\n                accuracy = (mask * label_matches).sum() / mask.sum()\n            else:\n                loss = loss.mean()\n                accuracy = torch.sum(label_matches) / label_matches.shape[0]\n\n        return DTrOCRLMHeadModelOutput(\n            loss=loss,\n            logits=logits,\n            accuracy=accuracy,\n            past_key_values=transformer_output.past_key_values\n        )\n\n    @torch.no_grad()\n    def generate(\n            self,\n            inputs: DTrOCRProcessorOutput,\n            processor: DTrOCRProcessor,\n            num_beams: int = 1,\n            use_cache: bool = True\n    ):\n        # params and configs\n        batch_size = inputs.input_ids.shape[0]\n        model_kwargs = {\n            'pixel_values': inputs.pixel_values,\n            'attention_mask': inputs.attention_mask,\n            'use_cache': use_cache\n        }\n        generation_config = GenerationConfig(\n            max_new_tokens=1,\n            pad_token_id=processor.tokenizer.pad_token_id,\n            eos_token_id=processor.tokenizer.eos_token_id,\n            bos_token_id=processor.tokenizer.bos_token_id,\n            num_beams=num_beams,\n            max_length=processor.tokenizer.model_max_length\n        )\n\n        # interleave input_ids with `num_beams` additional sequences per batch\n        input_ids, model_kwargs = self._expand_inputs_for_generation(\n            input_ids=inputs.input_ids,\n            expand_size=generation_config.num_beams,\n            **model_kwargs,\n        )\n\n        # prepare stopping criteria\n        prepared_stopping_criteria = self._get_stopping_criteria(\n            generation_config=generation_config,\n            processor=processor\n        )\n\n        if num_beams > 1:\n            # prepare beam search scorer\n            beam_scorer = BeamSearchScorer(\n                batch_size=batch_size,\n                num_beams=generation_config.num_beams,\n                length_penalty=generation_config.length_penalty,\n                do_early_stopping=generation_config.early_stopping,\n                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n                max_length=generation_config.max_length,\n            )\n\n            # run beam sample\n            result = self._beam_search(\n                input_ids,\n                beam_scorer,\n                logits_processor=LogitsProcessorList(),\n                stopping_criteria=prepared_stopping_criteria,\n                generation_config=generation_config,\n                **model_kwargs,\n            )\n\n        elif num_beams == 1:\n            result = self._sample(\n                input_ids,\n                logits_processor=LogitsProcessorList(),\n                stopping_criteria=prepared_stopping_criteria,\n                generation_config=generation_config,\n                **model_kwargs,\n            )\n        else:\n            raise ValueError(\"num_beams must be a positive integer.\")\n\n        return result\n\n    def _sample(\n        self,\n        input_ids: torch.Tensor,\n        logits_processor: LogitsProcessorList,\n        stopping_criteria: StoppingCriteriaList,\n        generation_config: GenerationConfig,\n        **model_kwargs,\n    ) -> torch.Tensor:\n        # init values\n        pad_token_id = generation_config.pad_token_id\n        has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n\n        # keep track of which sequences are already finished\n        batch_size = input_ids.shape[0]\n        unfinished_sequences = torch.ones(batch_size, dtype=torch.long)\n        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n\n        this_peer_finished = False\n        while not this_peer_finished:\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n            outputs = self(**model_inputs)\n\n            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first\n            # iteration (the clone itself is always small)\n            next_token_logits = outputs.logits[:, -1, :].clone()\n\n            # pre-process distribution\n            next_token_scores = logits_processor(input_ids, next_token_logits)\n\n            # token selection\n            next_tokens = torch.argmax(next_token_scores, dim=-1)\n\n            # finished sentences should have their next token be a padding token\n            if has_eos_stopping_criteria:\n                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n\n            # update generated ids, model inputs, and length for next step\n            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n\n            # update generated ids, model inputs, and length for next step\n            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs)\n\n            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, None)\n            this_peer_finished = unfinished_sequences.max() == 0\n\n            # This is needed to properly delete outputs.logits which may be very large for first iteration\n            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n            del outputs\n\n        return input_ids\n\n    def _beam_search(\n        self,\n        input_ids: torch.Tensor,\n        beam_scorer: BeamScorer,\n        logits_processor: LogitsProcessorList,\n        stopping_criteria: StoppingCriteriaList,\n        generation_config: GenerationConfig,\n        **model_kwargs,\n    ) -> torch.Tensor:\n        # init values\n        pad_token_id = generation_config.pad_token_id\n        eos_token_id = generation_config.eos_token_id\n\n        batch_size = len(beam_scorer._beam_hyps)\n        num_beams = beam_scorer.num_beams\n\n        batch_beam_size, cur_len = input_ids.shape\n        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n\n        if num_beams * batch_size != batch_beam_size:\n            raise ValueError(\n                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n            )\n\n        # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n        # of the first beam are considered to avoid sampling the exact same tokens across all beams.\n        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float)\n        beam_scores[:, 1:] = -1e9\n        beam_scores = beam_scores.view((batch_size * num_beams,))\n\n        this_peer_finished = False\n        decoder_prompt_len = input_ids.shape[-1]\n        while not this_peer_finished:\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n            outputs = self(**model_inputs)\n\n            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first\n            # iteration (the clone itself is always small)\n            next_token_logits = outputs.logits[:, -1, :].clone()\n            next_token_scores = nn.functional.log_softmax(\n                next_token_logits, dim=-1\n            )  # (batch_size * num_beams, vocab_size)\n\n            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n                next_token_scores_processed\n            )\n\n            # reshape for beam search\n            vocab_size = next_token_scores.shape[-1]\n            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n\n            # Beam token selection: pick 1 + eos_token_id.shape[0] next tokens for each beam so we have at least 1\n            # non eos token per beam.\n            n_tokens_to_keep = max(2, 1 + 1) * num_beams\n            next_token_scores, next_tokens = torch.topk(\n                next_token_scores, n_tokens_to_keep, dim=1, largest=True, sorted=True\n            )\n\n            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n            next_tokens = next_tokens % vocab_size\n\n            # stateless\n            beam_outputs = beam_scorer.process(\n                input_ids,\n                next_token_scores,\n                next_tokens,\n                next_indices,\n                pad_token_id=pad_token_id,\n                eos_token_id=eos_token_id,\n                decoder_prompt_len=decoder_prompt_len,\n            )\n\n            beam_scores = beam_outputs[\"next_beam_scores\"]\n            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n            beam_idx = beam_outputs[\"next_beam_indices\"]\n\n            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n\n            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs)\n\n            # This is needed to properly delete outputs.logits which may be very large for first iteration\n            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n            # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n            # (that way the memory peak does not include outputs.logits)\n            del outputs\n\n            if model_kwargs.get(\"past_key_values\", None) is not None:\n                model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n\n            # increase cur_len\n            cur_len = cur_len + 1\n\n            if beam_scorer.is_done or all(stopping_criteria(input_ids, None)):\n                this_peer_finished = True\n\n        sequence_outputs = beam_scorer.finalize(\n            input_ids,\n            beam_scores,\n            next_tokens,\n            next_indices,\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            max_length=stopping_criteria.max_length,\n            decoder_prompt_len=decoder_prompt_len,\n        )\n\n        return sequence_outputs[\"sequences\"]\n\n    def _get_stopping_criteria(\n        self,\n        generation_config: GenerationConfig,\n        processor: Optional[DTrOCRProcessor] = None,\n    ) -> StoppingCriteriaList:\n        criteria = StoppingCriteriaList()\n        if generation_config.max_length is not None:\n            max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n            criteria.append(\n                MaxLengthCriteria(\n                    max_length=generation_config.max_length,\n                    max_position_embeddings=max_position_embeddings,\n                )\n            )\n        if generation_config.max_time is not None:\n            criteria.append(MaxTimeCriteria(max_time=generation_config.max_time))\n        if generation_config.stop_strings is not None:\n            if processor is None:\n                raise ValueError(\n                    \"There are one or more stop strings, either in the arguments to `generate` or in the \"\n                    \"model's generation config, but we could not locate a tokenizer. When generating with \"\n                    \"stop strings, you must pass the model's tokenizer to the `tokenizer` argument of `generate`.\"\n                )\n            criteria.append(StopStringCriteria(\n                stop_strings=generation_config.stop_strings, tokenizer=processor.tokenizer)\n            )\n        if generation_config.eos_token_id is not None:\n            criteria.append(EosTokenCriteria(eos_token_id=generation_config.eos_token_id))\n        return criteria\n\n    @staticmethod\n    def _reorder_cache(\n            past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n    ) -> tuple[tuple[Tensor, ...], ...]:\n        \"\"\"\n        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n        beam_idx at every generation step.\n        \"\"\"\n        return tuple(\n            tuple(past_state.index_select(0, beam_idx) for past_state in layer_past)\n            for layer_past in past_key_values\n        )\n\n    @staticmethod\n    def _update_model_kwargs_for_generation(\n        outputs: DTrOCRLMHeadModelOutput,\n        model_kwargs: Dict[str, Any],\n        num_new_tokens: int = 1,\n    ) -> Dict[str, Any]:\n\n        # update cache\n        model_kwargs['past_key_values'] = outputs.past_key_values\n\n        # update attention mask\n        if \"attention_mask\" in model_kwargs:\n            attention_mask = model_kwargs[\"attention_mask\"]\n            model_kwargs[\"attention_mask\"] = torch.cat(\n                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n            )\n\n        if (\n            model_kwargs.get(\"use_cache\", True)\n            and \"cache_position\" in model_kwargs\n            and model_kwargs[\"cache_position\"] is not None\n        ):\n            model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + num_new_tokens\n\n        return model_kwargs\n\n    @staticmethod\n    def prepare_inputs_for_generation(\n        input_ids: torch.Tensor, past_key_values=None, **kwargs\n    ) -> Dict[str, Any]:\n        # Omit tokens covered by past_key_values\n        if past_key_values:\n            past_length = past_key_values[0][0].shape[2]\n\n            # Some generation methods already pass only the last input ID\n            if input_ids.shape[1] > past_length:\n                remove_prefix_length = past_length\n            else:\n                # Default to old behavior: keep only final ID\n                remove_prefix_length = input_ids.shape[1] - 1\n\n            input_ids = input_ids[:, remove_prefix_length:]\n\n        attention_mask = kwargs.get(\"attention_mask\", None)\n        position_ids = kwargs.get(\"position_ids\", None)\n\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1]:]\n        else:\n            position_ids = None\n\n        model_inputs = {\n            'input_ids': input_ids,\n            \"past_key_values\": past_key_values,\n            'pixel_values': kwargs['pixel_values'],\n            'use_cache': kwargs.get(\"use_cache\"),\n            'labels': kwargs.get(\"labels\"),\n            'attention_mask': attention_mask,\n            'position_ids': position_ids\n        }\n\n        return model_inputs\n\n    @staticmethod\n    def _get_initial_cache_position(input_ids, model_kwargs):\n        if not model_kwargs.get(\"use_cache\", True):\n            model_kwargs[\"cache_position\"] = None\n            return model_kwargs\n\n        model_kwargs[\"cache_position\"] = torch.arange(0, input_ids.shape[-1])\n        return model_kwargs\n\n    @staticmethod\n    def _expand_inputs_for_generation(\n        input_ids: Optional[torch.LongTensor],\n        expand_size: int = 1,\n        **model_kwargs,\n    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n        def _expand_dict_for_generation(dict_to_expand):\n            for key in dict_to_expand:\n                if (\n                        key != \"cache_position\"\n                        and dict_to_expand[key] is not None\n                        and isinstance(dict_to_expand[key], torch.Tensor)\n                ):\n                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n            return dict_to_expand\n\n        input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n        model_kwargs = _expand_dict_for_generation(model_kwargs)\n\n        return input_ids, model_kwargs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:19.007236Z","iopub.execute_input":"2024-09-21T09:32:19.007937Z","iopub.status.idle":"2024-09-21T09:32:20.133777Z","shell.execute_reply.started":"2024-09-21T09:32:19.007890Z","shell.execute_reply":"2024-09-21T09:32:20.132547Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\nwords_txt = []\nwith open('/kaggle/input/iam-handwriting-word-database/words_new.txt', 'r') as w_f:\n    lines = w_f.readlines()\n    for line in lines:\n        if line[0] == \"#\":\n            continue\n        spl = line.strip().split(\" \")\n        words_txt.append([spl[0], spl[1], spl[-1]])\n\n        \ndf = pd.DataFrame(words_txt, columns=['id', 'status', 'transcription'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:20.135041Z","iopub.execute_input":"2024-09-21T09:32:20.135348Z","iopub.status.idle":"2024-09-21T09:32:20.275652Z","shell.execute_reply.started":"2024-09-21T09:32:20.135314Z","shell.execute_reply":"2024-09-21T09:32:20.274632Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"               id status transcription\n0  a01-000u-00-00     ok             A\n1  a01-000u-00-01     ok          MOVE\n2  a01-000u-00-02     ok            to\n3  a01-000u-00-03     ok          stop\n4  a01-000u-00-04     ok           Mr.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>status</th>\n      <th>transcription</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a01-000u-00-00</td>\n      <td>ok</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a01-000u-00-01</td>\n      <td>ok</td>\n      <td>MOVE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a01-000u-00-02</td>\n      <td>ok</td>\n      <td>to</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a01-000u-00-03</td>\n      <td>ok</td>\n      <td>stop</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a01-000u-00-04</td>\n      <td>ok</td>\n      <td>Mr.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"with open('/kaggle/input/iam-splits/splits/train.uttlist') as fp:\n    train_ids = [line.replace('\\n', '') for line in fp.readlines()]\n\nwith open('/kaggle/input/iam-splits/splits/test.uttlist') as fp:\n    test_ids = [line.replace('\\n', '') for line in fp.readlines()]\n\nwith open('/kaggle/input/iam-splits/splits/validation.uttlist') as fp:\n    validation_ids = [line.replace('\\n', '') for line in fp.readlines()]\n\nprint(f\"Train size: {len(train_ids)}; Validation size: {len(validation_ids)}; Test size: {len(test_ids)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:20.276958Z","iopub.execute_input":"2024-09-21T09:32:20.277366Z","iopub.status.idle":"2024-09-21T09:32:20.299438Z","shell.execute_reply.started":"2024-09-21T09:32:20.277318Z","shell.execute_reply":"2024-09-21T09:32:20.298383Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Train size: 747; Validation size: 116; Test size: 336\n","output_type":"stream"}]},{"cell_type":"code","source":"import os \nl1_dir = \"/kaggle/input/iam-handwriting-word-database/iam_words/words\"\ndata_l1 = os.listdir(l1_dir)\ndata_l2 = []\nfor l1 in data_l1:\n    data_l2 += os.listdir(os.path.join(l1_dir, l1))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:20.300726Z","iopub.execute_input":"2024-09-21T09:32:20.301489Z","iopub.status.idle":"2024-09-21T09:32:20.723746Z","shell.execute_reply.started":"2024-09-21T09:32:20.301446Z","shell.execute_reply":"2024-09-21T09:32:20.722641Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for a, b, c in zip(train_ids, test_ids, validation_ids):\n    if a not in data_l2:\n        print(\"Not in:\", a)\n    if b not in data_l2:\n        print(\"Not in:\", b)\n    if c not in data_l2:\n        print(\"Not in:\", c)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:20.725397Z","iopub.execute_input":"2024-09-21T09:32:20.726108Z","iopub.status.idle":"2024-09-21T09:32:20.744948Z","shell.execute_reply.started":"2024-09-21T09:32:20.726063Z","shell.execute_reply":"2024-09-21T09:32:20.741024Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"l0_dir = '/kaggle/input/iam-handwriting-word-database/iam_words/words'\n\ntest_words = []\ntrain_words = []\nvalidation_words = []\n\nfor w in list(words_txt):\n    splitted = w[0].strip().split('-')\n    l2_part = '-'.join(splitted[:2])\n    id, status, transcription = w\n    filepath = os.path.join(l0_dir, splitted[0], l2_part, '-'.join(splitted) + \".png\", )\n    if l2_part in train_ids:\n        train_words.append([filepath, transcription])\n        \n    if l2_part in test_ids:\n        test_words.append([filepath, transcription])\n        \n    if l2_part in validation_ids:\n        validation_words.append([filepath, transcription])","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:20.750439Z","iopub.execute_input":"2024-09-21T09:32:20.753344Z","iopub.status.idle":"2024-09-21T09:32:22.176921Z","shell.execute_reply.started":"2024-09-21T09:32:20.753299Z","shell.execute_reply":"2024-09-21T09:32:22.175843Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test_words = np.array(test_words)\ntrain_words = np.array(train_words)\nvalidation_words = np.array(validation_words)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:22.178220Z","iopub.execute_input":"2024-09-21T09:32:22.178537Z","iopub.status.idle":"2024-09-21T09:32:22.254041Z","shell.execute_reply.started":"2024-09-21T09:32:22.178496Z","shell.execute_reply":"2024-09-21T09:32:22.252959Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nclass IAMDataset(Dataset):\n    def __init__(self, words, config: DTrOCRConfig):\n        super(IAMDataset, self).__init__()\n        self.words = words\n        self.processor = DTrOCRProcessor(config, add_eos_token=True, add_bos_token=True)\n        \n    def __len__(self):\n        return len(self.words)\n    \n    def __getitem__(self, item):\n        inputs = self.processor(\n            images=Image.open(self.words[item][0]).convert('RGB'),\n            texts=self.words[item][1],\n            padding='max_length',\n            return_labels=True,\n            return_tensors='pt',\n        )\n        return {\n            'pixel_values': inputs.pixel_values[0],\n            'input_ids': inputs.input_ids[0],\n            'attention_mask': inputs.attention_mask[0],\n            'labels': inputs.labels[0]\n        }\n\nconfig = DTrOCRConfig()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:22.255196Z","iopub.execute_input":"2024-09-21T09:32:22.255705Z","iopub.status.idle":"2024-09-21T09:32:22.264612Z","shell.execute_reply.started":"2024-09-21T09:32:22.255657Z","shell.execute_reply":"2024-09-21T09:32:22.263676Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nclass DTrOCRDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.train_data = IAMDataset(words=train_words, config=config)\n        self.validation_data = IAMDataset(words=validation_words, config=config)\n        self.test_data = IAMDataset(words=test_words, config=config)\n\n    def train_dataloader(self):\n        return DataLoader(self.train_data, batch_size=32, shuffle=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.validation_data, batch_size=32, shuffle=False)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:22.265895Z","iopub.execute_input":"2024-09-21T09:32:22.266260Z","iopub.status.idle":"2024-09-21T09:32:22.276168Z","shell.execute_reply.started":"2024-09-21T09:32:22.266219Z","shell.execute_reply":"2024-09-21T09:32:22.275083Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"torch.set_float32_matmul_precision(\"medium\")\n\nfrom pytorch_lightning.callbacks import EarlyStopping, Callback\n\nclass MyPrintingCallback(Callback):\n    def __init__(self):\n        super().__init__()\n\n    def on_train_start(self, trainer, pl_module):\n        print(\"Starting to train!\")\n\n    def on_train_end(self, trainer, pl_module):\n        print(\"Training is done.\")\n\ndm = DTrOCRDataModule()\n\nNUM_EPOCHS = 10\nACCELERATOR = \"gpu\"\nDEVICES = [0, 1]\nPRECISION = 16\n\nmodel = DTrOCRLMHeadModel(config)\n\ntrainer = pl.Trainer(\n    strategy='ddp_notebook',\n    accelerator=ACCELERATOR,\n    devices=DEVICES,\n    min_epochs=1,\n    max_epochs=NUM_EPOCHS,\n    precision=PRECISION,\n    callbacks=[MyPrintingCallback(), EarlyStopping(monitor=\"val_loss\")],\n)\n\ntrainer.fit(model, dm)\ntrainer.validate(model, dm)\ntrainer.test(model, dm)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:32:22.277367Z","iopub.execute_input":"2024-09-21T09:32:22.277737Z","iopub.status.idle":"2024-09-21T09:32:29.901632Z","shell.execute_reply.started":"2024-09-21T09:32:22.277696Z","shell.execute_reply":"2024-09-21T09:32:29.900151Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30aa0d3e0f2b40fc8d3c2c56b9eecb10"}},"metadata":{}},{"name":"stderr","text":"W0921 09:32:28.918000 138294338516800 torch/multiprocessing/spawn.py:146] Terminating process 103 via signal SIGTERM\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 34\u001b[0m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m DTrOCRLMHeadModel(config)\n\u001b[1;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     25\u001b[0m     strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddp_notebook\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39mACCELERATOR,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[MyPrintingCallback(), EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m trainer\u001b[38;5;241m.\u001b[39mvalidate(model, dm)\n\u001b[1;32m     36\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, dm)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:46\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:189\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    187\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    188\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n","\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 76, in _wrap\n    fn(i, *args)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n    results = self._run_stage()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n    self._run_sanity_check()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n    val_loop.run()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n    return loop_run(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 410, in validation_step\n    return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 640, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 633, in wrapped_forward\n    out = method(*_args, **_kwargs)\n  File \"/tmp/ipykernel_36/559489690.py\", line 158, in validation_step\n    outputs = self.forward(**inputs)\n  File \"/tmp/ipykernel_36/559489690.py\", line 182, in forward\n    transformer_output = self.transformer(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_36/559489690.py\", line 85, in forward\n    position_embeddings = self.positional_embedding(position_ids)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 164, in forward\n    return F.embedding(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 2267, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"],"ename":"ProcessRaisedException","evalue":"\n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 76, in _wrap\n    fn(i, *args)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n    results = self._run_stage()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n    self._run_sanity_check()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n    val_loop.run()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n    return loop_run(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 410, in validation_step\n    return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 640, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 633, in wrapped_forward\n    out = method(*_args, **_kwargs)\n  File \"/tmp/ipykernel_36/559489690.py\", line 158, in validation_step\n    outputs = self.forward(**inputs)\n  File \"/tmp/ipykernel_36/559489690.py\", line 182, in forward\n    transformer_output = self.transformer(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_36/559489690.py\", line 85, in forward\n    position_embeddings = self.positional_embedding(position_ids)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 164, in forward\n    return F.embedding(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 2267, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}