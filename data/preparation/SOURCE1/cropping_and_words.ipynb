{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWBgWLxQVefK",
        "outputId": "c06f9cf9-b3b7-4d9d-85fa-38c7c14d0f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting english-words\n",
            "  Downloading english-words-2.0.1.tar.gz (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: english-words\n",
            "  Building wheel for english-words (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for english-words: filename=english_words-2.0.1-py3-none-any.whl size=8196236 sha256=adb34b76b8026d3b59725fa682b56c5b648b983c9929c65b04a9c6d5cb0665d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/e6/d9/16a123647999fe535f03a36e7af23eef203736d84c7ca25b0b\n",
            "Successfully built english-words\n",
            "Installing collected packages: english-words\n",
            "Successfully installed english-words-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install english-words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "archive_path = '/content/drive/MyDrive/Datasets/SOURCE1/by_class.zip'\n",
        "with zipfile.ZipFile(archive_path, 'r') as arc_f:\n",
        "  arc_f.extractall('/content/')"
      ],
      "metadata": {
        "id": "ZcRK-6mM3wD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def get_grayscale_image(filepath):\n",
        "  img = Image.open(filepath).convert('L')\n",
        "  return np.array(img)\n",
        "\n",
        "def crop_grayscale(img):\n",
        "  _, bin_img = cv2.threshold(img, 128, 255, type=cv2.THRESH_BINARY_INV)\n",
        "  contours, _ = cv2.findContours(bin_img, mode=cv2.RETR_EXTERNAL,\n",
        "                              method=cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  if not contours:\n",
        "    return img\n",
        "\n",
        "  y_min, y_max, x_min, x_max = np.inf, -np.inf, np.inf, -np.inf\n",
        "\n",
        "  for contour in contours:\n",
        "    x, y, w, h = cv2.boundingRect(contour)\n",
        "    y_min = min(y, y_min)\n",
        "    y_max = max(y+h, y_max)\n",
        "    x_min = min(x, x_min)\n",
        "    x_max = max(x+w, x_max)\n",
        "\n",
        "  croppped_img = bin_img[y_min:y_max, x_min:x_max]\n",
        "\n",
        "  return croppped_img"
      ],
      "metadata": {
        "id": "aPvfgmkl4EvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1hRN8BvVgwl"
      },
      "outputs": [],
      "source": [
        "from english_words import get_english_words_set\n",
        "words_list = list(get_english_words_set(['web2']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PgshErSAVlax"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "import time\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "train_dir = \"/content/by_class/train\"\n",
        "classes_imgs = {}\n",
        "classes_lengths = {}\n",
        "\n",
        "for c in os.listdir(train_dir):\n",
        "  classes_imgs[c] = os.listdir(os.path.join(train_dir, c))\n",
        "  classes_lengths[c] = len(classes_imgs[c])\n",
        "\n",
        "def generate_word(word, output_path):\n",
        "    char_imgs = []\n",
        "    word = list(word.strip())\n",
        "    max_height = 0\n",
        "    widths = []\n",
        "    for c in word:\n",
        "        class_dir = os.path.join(train_dir, c)\n",
        "        idx = random.randint(1, classes_lengths[c])\n",
        "\n",
        "        img = get_grayscale_image(os.path.join(class_dir, random.choice(classes_imgs[c])))\n",
        "        char = Image.fromarray(crop_grayscale(img), mode='L').convert('RGB')\n",
        "        max_height = max(max_height, char.size[1])\n",
        "        widths.append(char.size[0])\n",
        "        char_imgs.append(char)\n",
        "\n",
        "    padding_range = (0, 4)\n",
        "    padded_chars = []\n",
        "    random_paddings = [random.randint(*padding_range) for i in range(len(char_imgs) - 1)] + [0]\n",
        "\n",
        "    for char_idx in range(len(char_imgs)):\n",
        "        char_img = char_imgs[char_idx]\n",
        "        vert_padding = (max_height - char_img.size[1]) // 2 + 3\n",
        "\n",
        "        horz_padding = random_paddings[char_idx]\n",
        "\n",
        "        pad_img = ImageOps.expand(ImageOps.invert(char_img), (0, vert_padding, horz_padding, vert_padding), (255, 255, 255))\n",
        "\n",
        "        padded_chars.append(pad_img)\n",
        "\n",
        "    new_im = Image.new('RGB', (sum(widths) + sum(random_paddings), max_height + 6), color=(255, 255, 255))\n",
        "    x_offset = 0\n",
        "    for im in padded_chars:\n",
        "      new_im.paste(im, (x_offset, 0))\n",
        "      x_offset += im.size[0]\n",
        "    new_im.save(output_path)\n",
        "\n",
        "\n",
        "def generate_filename(word, frmt='jpg'):\n",
        "    return word + '_' + str(time.time()) + '_' + str(random.randint(100, 999)) + '.' + frmt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oo4AUSOHVl7u"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/words/train/'\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "for idx in range(50000):\n",
        "  random_w_idx = random.randint(0, len(words_list) - 1)\n",
        "  word = words_list[random_w_idx]\n",
        "  filename = generate_filename(word)\n",
        "  while os.path.exists(os.path.join(out_dir, filename)):\n",
        "    filename = generate_filename(word)\n",
        "  generate_word(word, os.path.join(out_dir, filename))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dsoFzaq8Z28r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6cdf5894-333b-40eb-dde2-af67b2003dd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/words_archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"/content/words_archive\", \"zip\", out_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3CMbEOEnBjUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvZs1q-vBm-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}