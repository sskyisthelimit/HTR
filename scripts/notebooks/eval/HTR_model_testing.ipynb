{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hprxLBFekNg"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics lightning torchmetrics wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "yolo_model_dir = '/content/yolo_model'\n",
        "os.makedirs(yolo_model_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Models/words-yolov5/words-yolo.zip', 'r') as f:\n",
        "  f.extractall(yolo_model_dir)"
      ],
      "metadata": {
        "id": "fK3UrQOIfTxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import shutil\n",
        "\n",
        "SOURCE2_DIR = '/content/SOURCE2/'\n",
        "SOURCE3_DIR = '/content/SOURCE3/'\n",
        "\n",
        "os.makedirs(SOURCE2_DIR, exist_ok=True)\n",
        "os.makedirs(SOURCE3_DIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Datasets/SOURCE2/detection.zip', 'r') as zf:\n",
        "  zf.extractall(SOURCE2_DIR)\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Datasets/SOURCE3/detection.zip', 'r') as zf:\n",
        "  zf.extractall(SOURCE3_DIR)"
      ],
      "metadata": {
        "id": "t6SY0tzsjnaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "source3_test_pages = [os.path.join(SOURCE3_DIR, 'test', f) for f in os.listdir(os.path.join(SOURCE3_DIR, 'test'))]\n",
        "\n",
        "source3_test_annotations = os.path.join(SOURCE3_DIR, 'test_annotations.json')\n",
        "\n",
        "source2_test_pages = [os.path.join(SOURCE2_DIR, 'test', f) for f in os.listdir(os.path.join(SOURCE2_DIR, 'test'))]\n",
        "\n",
        "source2_test_annotations = os.path.join(SOURCE2_DIR, 'test_annotations.json')\n",
        "\n",
        "test_pages = source3_test_pages + source2_test_pages"
      ],
      "metadata": {
        "id": "pHqZ6x3Ij1YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_source2_annotation(img_id, json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        annotations = json.loads(f.read())\n",
        "\n",
        "    words = annotations[img_id][\"words\"]\n",
        "\n",
        "    bboxes = [w['bbox'] for w in words]\n",
        "    labels = [0 for i in range(len(words))]\n",
        "\n",
        "    return bboxes, labels\n",
        "\n",
        "def get_source3_annotation(img_id, json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        annotations = json.loads(f.read())\n",
        "\n",
        "    regions_contents = annotations[img_id][\"regions_contents\"]\n",
        "\n",
        "    bboxes = []\n",
        "    for rc in regions_contents:\n",
        "        bboxes += rc['bboxes']\n",
        "\n",
        "    labels = [0 for i in range(len(bboxes))]\n",
        "\n",
        "    return bboxes, labels\n",
        "\n",
        "def parse_annotations(pages, source2_annotations, source3_annotations, annotations):\n",
        "    for page_path in pages:\n",
        "        page_id = os.path.basename(page_path[:-4])\n",
        "        if SOURCE2_DIR in page_path:\n",
        "            bboxes, labels = get_source2_annotation(page_id, source2_annotations)\n",
        "            if page_id not in [*annotations]:\n",
        "                annotations[page_id] = {\"bboxes\": bboxes, \"labels\": labels}\n",
        "            else:\n",
        "                raise ValueError(f\"Page with id: {page_id} is already in annotations\")\n",
        "\n",
        "        elif SOURCE3_DIR in page_path:\n",
        "            bboxes, labels = get_source3_annotation(page_id, source3_annotations)\n",
        "            if page_id not in [*annotations]:\n",
        "                annotations[page_id] = {\"bboxes\": bboxes, \"labels\": labels}\n",
        "            else:\n",
        "                raise ValueError(f\"Page with id: {page_id} is already in annotations\")"
      ],
      "metadata": {
        "id": "9ODYgN2Yj9dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_annotations = {}\n",
        "parse_annotations(test_pages, source2_test_annotations,\n",
        "                  source3_test_annotations, test_annotations)"
      ],
      "metadata": {
        "id": "Yq_0wMRjkJ3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Union, Tuple, List, Literal\n",
        "\n",
        "\n",
        "class DTrOCRConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        gpt2_hf_model: str = 'openai-community/gpt2',\n",
        "        vit_hf_model: str = 'google/vit-base-patch16-224',\n",
        "        vocab_size: Optional[int] = 50257,\n",
        "        max_position_embeddings: Optional[int] = 256,\n",
        "        hidden_size: Optional[int] = 768,\n",
        "        num_hidden_layers: Optional[int] = 8,\n",
        "        num_attention_heads: Optional[int] = 12,\n",
        "        patch_size: Optional[Union[Tuple[int], List[int]]] = (4, 8),      # (height, width)\n",
        "        image_size: Optional[Union[Tuple[int], List[int]]] = (32, 128),   # (height, width)\n",
        "        num_channels: Optional[int] = 3,\n",
        "        resid_pdrop: Optional[float] = 0.1,\n",
        "        embd_pdrop: Optional[float] = 0.1,\n",
        "        attn_pdrop: Optional[float] = 0.1,\n",
        "        layer_norm_epsilon: Optional[float] = 1e-5,\n",
        "        attn_implementation: Literal['sdpa', 'flash_attention_2'] = 'sdpa'\n",
        "    ):\n",
        "        self.gpt2_hf_model = gpt2_hf_model\n",
        "        self.vit_hf_model = vit_hf_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.patch_size = patch_size\n",
        "        self.image_size = image_size\n",
        "        self.num_channels = num_channels\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.resid_pdrop = resid_pdrop\n",
        "        self.embd_pdrop = embd_pdrop\n",
        "        self.attn_pdrop = attn_pdrop\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n",
        "        self._attn_implementation = attn_implementation\n",
        "\n",
        "        # other GPT2 config values\n",
        "        self.n_inner = None\n",
        "        self.scale_attn_weights = True\n",
        "        self.scale_attn_by_inverse_layer_idx = False\n",
        "        self.reorder_and_upcast_attn = False\n",
        "        self.add_cross_attention = False\n",
        "        self.activation_function = \"gelu_new\"\n"
      ],
      "metadata": {
        "id": "uAy-sgpqxfSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union, List\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DTrOCRModelOutput:\n",
        "    hidden_states: torch.FloatTensor\n",
        "    past_key_values: torch.FloatTensor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DTrOCRLMHeadModelOutput:\n",
        "    logits: torch.FloatTensor\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    accuracy: Optional[torch.FloatTensor] = None\n",
        "    past_key_values: Optional[torch.FloatTensor] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DTrOCRProcessorOutput:\n",
        "    pixel_values: Optional[torch.FloatTensor] = None\n",
        "    input_ids: Optional[Union[torch.LongTensor, np.ndarray, List[int]]] = None\n",
        "    attention_mask: Optional[Union[torch.FloatTensor, np.ndarray, List[int]]] = None\n",
        "    labels: Optional[Union[torch.LongTensor, np.ndarray, List[int]]] = None\n"
      ],
      "metadata": {
        "id": "-3plFtGqxfKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, AutoImageProcessor\n",
        "\n",
        "from PIL import Image\n",
        "from typing import List, Union\n",
        "\n",
        "class DTrOCRProcessor:\n",
        "    def __init__(self, config: DTrOCRConfig, add_bos_token: bool = False, add_eos_token: bool = False):\n",
        "        self.vit_processor = AutoImageProcessor.from_pretrained(\n",
        "            config.vit_hf_model,\n",
        "            size={\n",
        "                \"height\": config.image_size[0],\n",
        "                'width': config.image_size[1]\n",
        "            },\n",
        "            use_fast=True\n",
        "        )\n",
        "        self.tokeniser = GPT2Tokenizer.from_pretrained(\n",
        "            config.gpt2_hf_model,\n",
        "            add_bos_token=add_bos_token,\n",
        "            model_max_length=config.max_position_embeddings - int(\n",
        "                (config.image_size[0] / config.patch_size[0]) * (config.image_size[1] / config.patch_size[1])\n",
        "            )\n",
        "        )\n",
        "        self.tokeniser.pad_token = self.tokeniser.bos_token\n",
        "        self.tokeniser.add_eos_token = add_eos_token\n",
        "\n",
        "        # Bind a new method to gpt2_tokeniser\n",
        "        self.tokeniser.build_inputs_with_special_tokens = modified_build_inputs_with_special_tokens.__get__(\n",
        "            self.tokeniser\n",
        "        )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        images: Union[Image.Image, List[Image.Image]] = None,\n",
        "        texts: Union[str, List[str]] = None,\n",
        "        return_labels: bool = False,\n",
        "        input_data_format: str = 'channels_last',\n",
        "        padding: Union[bool, str] = False,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ) -> DTrOCRProcessorOutput:\n",
        "        text_inputs = self.tokeniser(\n",
        "            texts, padding=padding, *args, **kwargs\n",
        "        ) if texts is not None else None\n",
        "\n",
        "        image_inputs = self.vit_processor(\n",
        "            images, input_data_format=input_data_format, *args, **kwargs\n",
        "        ) if images is not None else None\n",
        "\n",
        "        return DTrOCRProcessorOutput(\n",
        "            pixel_values=image_inputs[\"pixel_values\"] if images is not None else None,\n",
        "            input_ids=text_inputs['input_ids'] if texts is not None else None,\n",
        "            attention_mask=text_inputs['attention_mask'] if texts is not None else None,\n",
        "            labels=text_inputs['input_ids'] if texts is not None and return_labels else None\n",
        "        )\n",
        "\n",
        "\n",
        "def modified_build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "    if self.add_bos_token:\n",
        "        bos_token_ids = [self.bos_token_id]\n",
        "    else:\n",
        "        bos_token_ids = []\n",
        "\n",
        "    if self.add_eos_token:\n",
        "        eos_token_ids = [self.eos_token_id]\n",
        "    else:\n",
        "        eos_token_ids = []\n",
        "\n",
        "    output = bos_token_ids + token_ids_0 + eos_token_ids\n",
        "\n",
        "    if token_ids_1 is None:\n",
        "        return output\n",
        "\n",
        "    return output + bos_token_ids + token_ids_1\n"
      ],
      "metadata": {
        "id": "BKhjR4Qyxe_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from typing import Optional, Tuple, Dict, Any\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from transformers.models.vit.modeling_vit import ViTPatchEmbeddings\n",
        "from transformers.generation.logits_process import LogitsProcessorList\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Model\n",
        "from transformers.generation.configuration_utils import GenerationConfig\n",
        "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask_for_sdpa\n",
        "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer\n",
        "from transformers.generation.stopping_criteria import (\n",
        "    EosTokenCriteria,\n",
        "    MaxLengthCriteria,\n",
        "    MaxTimeCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    StopStringCriteria,\n",
        ")\n",
        "\n",
        "class DTrOCRModel(pl.LightningModule):\n",
        "    def __init__(self, config: DTrOCRConfig):\n",
        "        super().__init__()\n",
        "        # embeddings\n",
        "        self.patch_embeddings = ViTPatchEmbeddings(config)\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.positional_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
        "        self.dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        self._attn_implementation = config._attn_implementation\n",
        "\n",
        "        # initialise GPT-2 weights from Hugging Face\n",
        "        self.initialise_weights(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.Tensor,\n",
        "        input_ids: torch.LongTensor,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        use_cache: Optional[bool] = False,\n",
        "    ) -> DTrOCRModelOutput:\n",
        "        device = input_ids.device if input_ids is not None else input_ids.device\n",
        "        input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
        "\n",
        "        # past key values\n",
        "        if past_key_values is None:\n",
        "            past_length = 0\n",
        "            past_key_values = tuple([None] * len(self.hidden_layers))\n",
        "        else:\n",
        "            past_length = past_key_values[0][0].size(-2)\n",
        "\n",
        "        patch_embeddings = self.patch_embeddings(pixel_values) if past_length == 0 else None\n",
        "        token_embeddings = self.token_embedding(input_ids)\n",
        "\n",
        "        if patch_embeddings is not None:\n",
        "            patch_and_token_embeddings = torch.concat([patch_embeddings, token_embeddings], dim=-2)\n",
        "        else:\n",
        "            patch_and_token_embeddings = token_embeddings\n",
        "        input_shape = patch_and_token_embeddings.shape\n",
        "\n",
        "        if position_ids is None or past_length == 0:\n",
        "            position_ids = torch.arange(past_length, input_shape[1] + past_length, dtype=torch.long, device=device)\n",
        "            position_ids = position_ids.unsqueeze(0)\n",
        "        else:\n",
        "            position_ids = torch.ones_like(position_ids, device=position_ids.device) * past_length\n",
        "        position_embeddings = self.positional_embedding(position_ids)\n",
        "\n",
        "        hidden_states = patch_and_token_embeddings + position_embeddings\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        # attention mask\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = torch.concat(\n",
        "                [\n",
        "                    torch.ones(\n",
        "                        attention_mask.shape[0],\n",
        "                        patch_embeddings.shape[-2] if patch_embeddings is not None else past_length,\n",
        "                        dtype=attention_mask.dtype,\n",
        "                        device=attention_mask.device\n",
        "                    ),\n",
        "                    attention_mask\n",
        "                ], dim=-1\n",
        "            )\n",
        "            if self._attn_implementation == \"flash_attention_2\":\n",
        "                attention_mask = attention_mask if 0 in attention_mask else None\n",
        "            else:\n",
        "                attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n",
        "                    attention_mask=attention_mask,\n",
        "                    input_shape=(input_shape[0], input_shape[-2]),\n",
        "                    inputs_embeds=patch_and_token_embeddings,\n",
        "                    past_key_values_length=past_length,\n",
        "                )\n",
        "\n",
        "        presents = () if use_cache else None\n",
        "        for hidden_layer, layer_past in zip(self.hidden_layers, past_key_values):\n",
        "            outputs = hidden_layer(\n",
        "                hidden_states,\n",
        "                layer_past=layer_past,\n",
        "                attention_mask=attention_mask,\n",
        "                use_cache=use_cache\n",
        "            )\n",
        "            hidden_states = outputs[0]\n",
        "            if use_cache is True:\n",
        "                presents = presents + (outputs[1],)\n",
        "\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "\n",
        "        return DTrOCRModelOutput(hidden_states=hidden_states, past_key_values=presents)\n",
        "\n",
        "    def initialise_weights(self, config: DTrOCRConfig) -> None:\n",
        "        # load pre-trained GPT-2\n",
        "        pretrained_gpt2 = GPT2Model.from_pretrained(config.gpt2_hf_model)\n",
        "\n",
        "        # copy hidden layer weights\n",
        "        for hidden_layer, pretrained_hidden_layer in zip(self.hidden_layers, pretrained_gpt2.h):\n",
        "            hidden_layer.load_state_dict(pretrained_hidden_layer.state_dict())\n",
        "\n",
        "        # token embeddings\n",
        "        self.token_embedding.load_state_dict(pretrained_gpt2.wte.state_dict())\n",
        "\n",
        "\n",
        "class DTrOCRLMHeadModel(pl.LightningModule):\n",
        "    def __init__(self, config: DTrOCRConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = DTrOCRModel(config)\n",
        "        self.language_model_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        image_size, patch_size = config.image_size, config.patch_size\n",
        "\n",
        "        self.image_embedding_length = int((image_size[0] / patch_size[0]) * (image_size[1] / patch_size[1]))\n",
        "\n",
        "        self.val_cer = torchmetrics.text.CharErrorRate()\n",
        "        self.test_cer = torchmetrics.text.CharErrorRate()\n",
        "        # To log in wandb\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def training_step(self, inputs):\n",
        "        outputs = self.forward(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"train_loss\": loss,\n",
        "            },\n",
        "            commit=True\n",
        "        )\n",
        "\n",
        "        return {\"loss\": loss, \"scores\": outputs.accuracy, \"y\": outputs.logits}\n",
        "\n",
        "    def validation_step(self, inputs):\n",
        "        outputs = self.forward(**inputs)\n",
        "\n",
        "        labels = inputs['labels']\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = outputs.loss\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"val_loss\": loss,\n",
        "                \"val_accuracy\": outputs.accuracy\n",
        "            },\n",
        "            commit=True\n",
        "        )\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        self.val_cer.update(preds, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        outputs = self.forward(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        labels = inputs['labels']\n",
        "        logits = outputs.logits\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"val_loss\": loss,\n",
        "                \"val_accuracy\": outputs.accuracy\n",
        "            },\n",
        "            commit=True\n",
        "        )\n",
        "\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        self.test_cer.update(preds, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"y\": outputs.logits}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_cer_value = self.val_cer.compute()\n",
        "        wandb.log({'val_cer': val_cer_value}, commit=True)\n",
        "\n",
        "        self.val_cer.reset()\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        test_cer_value = self.test_cer.compute()\n",
        "        wandb.log({'test_cer': test_cer_value}, commit=True)\n",
        "\n",
        "        self.test_cer.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(params=self.parameters(), lr=1e-4)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.Tensor,\n",
        "        input_ids: torch.LongTensor,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "    ) -> DTrOCRLMHeadModelOutput:\n",
        "        transformer_output = self.transformer(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            position_ids=position_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=use_cache\n",
        "        )\n",
        "        logits = self.language_model_head(transformer_output.hidden_states)\n",
        "\n",
        "        loss, accuracy = None, None\n",
        "        if labels is not None:\n",
        "            labels = labels.to(logits.device)\n",
        "\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., self.image_embedding_length:-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "            loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            label_matches = shift_labels.view(-1) == torch.argmax(\n",
        "                torch.nn.functional.softmax(shift_logits.view(-1, shift_logits.size(-1)), dim=-1), dim=-1\n",
        "            )\n",
        "\n",
        "            # reduce loss\n",
        "            if attention_mask is not None:\n",
        "                mask = attention_mask[..., 1:].reshape(-1)\n",
        "\n",
        "                loss = (mask * loss).sum() / mask.sum()\n",
        "                accuracy = (mask * label_matches).sum() / mask.sum()\n",
        "            else:\n",
        "                loss = loss.mean()\n",
        "                accuracy = torch.sum(label_matches) / label_matches.shape[0]\n",
        "\n",
        "        return DTrOCRLMHeadModelOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            accuracy=accuracy,\n",
        "            past_key_values=transformer_output.past_key_values\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "            self,\n",
        "            inputs: DTrOCRProcessorOutput,\n",
        "            processor: DTrOCRProcessor,\n",
        "            num_beams: int = 1,\n",
        "            use_cache: bool = True\n",
        "    ):\n",
        "        # params and configs\n",
        "        batch_size = inputs.input_ids.shape[0]\n",
        "        model_kwargs = {\n",
        "            'pixel_values': inputs.pixel_values,\n",
        "            'attention_mask': inputs.attention_mask,\n",
        "            'use_cache': use_cache\n",
        "        }\n",
        "        generation_config = GenerationConfig(\n",
        "            max_new_tokens=1,\n",
        "            pad_token_id=processor.tokeniser.pad_token_id,\n",
        "            eos_token_id=processor.tokeniser.eos_token_id,\n",
        "            bos_token_id=processor.tokeniser.bos_token_id,\n",
        "            num_beams=num_beams,\n",
        "            max_length=processor.tokeniser.model_max_length\n",
        "        )\n",
        "\n",
        "        # interleave input_ids with `num_beams` additional sequences per batch\n",
        "        input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
        "            input_ids=inputs.input_ids,\n",
        "            expand_size=generation_config.num_beams,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "        # prepare stopping criteria\n",
        "        prepared_stopping_criteria = self._get_stopping_criteria(\n",
        "            generation_config=generation_config,\n",
        "            processor=processor\n",
        "        )\n",
        "\n",
        "        if num_beams > 1:\n",
        "            # prepare beam search scorer\n",
        "            beam_scorer = BeamSearchScorer(\n",
        "                batch_size=batch_size,\n",
        "                num_beams=generation_config.num_beams,\n",
        "                device=inputs.input_ids.device,\n",
        "                length_penalty=generation_config.length_penalty,\n",
        "                do_early_stopping=generation_config.early_stopping,\n",
        "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
        "                max_length=generation_config.max_length,\n",
        "            )\n",
        "\n",
        "            # run beam sample\n",
        "            result = self._beam_search(\n",
        "                input_ids,\n",
        "                beam_scorer,\n",
        "                logits_processor=LogitsProcessorList(),\n",
        "                stopping_criteria=prepared_stopping_criteria,\n",
        "                generation_config=generation_config,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "\n",
        "        elif num_beams == 1:\n",
        "            result = self._sample(\n",
        "                input_ids,\n",
        "                logits_processor=LogitsProcessorList(),\n",
        "                stopping_criteria=prepared_stopping_criteria,\n",
        "                generation_config=generation_config,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"num_beams must be a positive integer.\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _sample(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        logits_processor: LogitsProcessorList,\n",
        "        stopping_criteria: StoppingCriteriaList,\n",
        "        generation_config: GenerationConfig,\n",
        "        **model_kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        # init values\n",
        "        pad_token_id = generation_config.pad_token_id\n",
        "        has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n",
        "\n",
        "        # keep track of which sequences are already finished\n",
        "        batch_size = input_ids.shape[0]\n",
        "        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
        "        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
        "\n",
        "        this_peer_finished = False\n",
        "        while not this_peer_finished:\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "            outputs = self(**model_inputs)\n",
        "\n",
        "            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first\n",
        "            # iteration (the clone itself is always small)\n",
        "            next_token_logits = outputs.logits[:, -1, :].clone()\n",
        "\n",
        "            # pre-process distribution\n",
        "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
        "\n",
        "            # token selection\n",
        "            next_tokens = torch.argmax(next_token_scores, dim=-1)\n",
        "\n",
        "            # finished sentences should have their next token be a padding token\n",
        "            if has_eos_stopping_criteria:\n",
        "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "            # update generated ids, model inputs, and length for next step\n",
        "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "\n",
        "            # update generated ids, model inputs, and length for next step\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs)\n",
        "\n",
        "            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, None)\n",
        "            this_peer_finished = unfinished_sequences.max() == 0\n",
        "\n",
        "            # This is needed to properly delete outputs.logits which may be very large for first iteration\n",
        "            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n",
        "            del outputs\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "    def _beam_search(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        beam_scorer: BeamScorer,\n",
        "        logits_processor: LogitsProcessorList,\n",
        "        stopping_criteria: StoppingCriteriaList,\n",
        "        generation_config: GenerationConfig,\n",
        "        **model_kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        # init values\n",
        "        pad_token_id = generation_config.pad_token_id\n",
        "        eos_token_id = generation_config.eos_token_id\n",
        "\n",
        "        batch_size = len(beam_scorer._beam_hyps)\n",
        "        num_beams = beam_scorer.num_beams\n",
        "\n",
        "        batch_beam_size, cur_len = input_ids.shape\n",
        "        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
        "\n",
        "        if num_beams * batch_size != batch_beam_size:\n",
        "            raise ValueError(\n",
        "                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
        "            )\n",
        "\n",
        "        # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n",
        "        # of the first beam are considered to avoid sampling the exact same tokens across all beams.\n",
        "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
        "        beam_scores[:, 1:] = -1e9\n",
        "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
        "\n",
        "        this_peer_finished = False\n",
        "        decoder_prompt_len = input_ids.shape[-1]\n",
        "        while not this_peer_finished:\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "            outputs = self(**model_inputs)\n",
        "\n",
        "            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first\n",
        "            # iteration (the clone itself is always small)\n",
        "            next_token_logits = outputs.logits[:, -1, :].clone()\n",
        "            next_token_scores = nn.functional.log_softmax(\n",
        "                next_token_logits, dim=-1\n",
        "            )  # (batch_size * num_beams, vocab_size)\n",
        "\n",
        "            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n",
        "            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n",
        "                next_token_scores_processed\n",
        "            )\n",
        "\n",
        "            # reshape for beam search\n",
        "            vocab_size = next_token_scores.shape[-1]\n",
        "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
        "\n",
        "            # Beam token selection: pick 1 + eos_token_id.shape[0] next tokens for each beam so we have at least 1\n",
        "            # non eos token per beam.\n",
        "            n_tokens_to_keep = max(2, 1 + 1) * num_beams\n",
        "            next_token_scores, next_tokens = torch.topk(\n",
        "                next_token_scores, n_tokens_to_keep, dim=1, largest=True, sorted=True\n",
        "            )\n",
        "\n",
        "            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
        "            next_tokens = next_tokens % vocab_size\n",
        "\n",
        "            # stateless\n",
        "            beam_outputs = beam_scorer.process(\n",
        "                input_ids,\n",
        "                next_token_scores,\n",
        "                next_tokens,\n",
        "                next_indices,\n",
        "                pad_token_id=pad_token_id,\n",
        "                eos_token_id=eos_token_id,\n",
        "                decoder_prompt_len=decoder_prompt_len,\n",
        "            )\n",
        "\n",
        "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
        "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
        "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
        "\n",
        "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs)\n",
        "\n",
        "            # This is needed to properly delete outputs.logits which may be very large for first iteration\n",
        "            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n",
        "            # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n",
        "            # (that way the memory peak does not include outputs.logits)\n",
        "            del outputs\n",
        "\n",
        "            if model_kwargs.get(\"past_key_values\", None) is not None:\n",
        "                model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n",
        "\n",
        "            # increase cur_len\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            if beam_scorer.is_done or all(stopping_criteria(input_ids, None)):\n",
        "                this_peer_finished = True\n",
        "\n",
        "        sequence_outputs = beam_scorer.finalize(\n",
        "            input_ids,\n",
        "            beam_scores,\n",
        "            next_tokens,\n",
        "            next_indices,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            max_length=stopping_criteria.max_length,\n",
        "            decoder_prompt_len=decoder_prompt_len,\n",
        "        )\n",
        "\n",
        "        return sequence_outputs[\"sequences\"]\n",
        "\n",
        "    def _get_stopping_criteria(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        processor: Optional[DTrOCRProcessor] = None,\n",
        "    ) -> StoppingCriteriaList:\n",
        "        criteria = StoppingCriteriaList()\n",
        "        if generation_config.max_length is not None:\n",
        "            max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n",
        "            criteria.append(\n",
        "                MaxLengthCriteria(\n",
        "                    max_length=generation_config.max_length,\n",
        "                    max_position_embeddings=max_position_embeddings,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.max_time is not None:\n",
        "            criteria.append(MaxTimeCriteria(max_time=generation_config.max_time))\n",
        "        if generation_config.stop_strings is not None:\n",
        "            if processor is None:\n",
        "                raise ValueError(\n",
        "                    \"There are one or more stop strings, either in the arguments to `generate` or in the \"\n",
        "                    \"model's generation config, but we could not locate a tokenizer. When generating with \"\n",
        "                    \"stop strings, you must pass the model's tokenizer to the `tokenizer` argument of `generate`.\"\n",
        "                )\n",
        "            criteria.append(StopStringCriteria(\n",
        "                stop_strings=generation_config.stop_strings, tokenizer=processor.tokeniser)\n",
        "            )\n",
        "        if generation_config.eos_token_id is not None:\n",
        "            criteria.append(EosTokenCriteria(eos_token_id=generation_config.eos_token_id))\n",
        "        return criteria\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(\n",
        "            past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n",
        "    ) -> tuple[tuple[Tensor, ...], ...]:\n",
        "        \"\"\"\n",
        "        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n",
        "        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n",
        "        beam_idx at every generation step.\n",
        "        \"\"\"\n",
        "        return tuple(\n",
        "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
        "            for layer_past in past_key_values\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _update_model_kwargs_for_generation(\n",
        "        outputs: DTrOCRLMHeadModelOutput,\n",
        "        model_kwargs: Dict[str, Any],\n",
        "        num_new_tokens: int = 1,\n",
        "    ) -> Dict[str, Any]:\n",
        "\n",
        "        # update cache\n",
        "        model_kwargs['past_key_values'] = outputs.past_key_values\n",
        "\n",
        "        # update attention mask\n",
        "        if \"attention_mask\" in model_kwargs:\n",
        "            attention_mask = model_kwargs[\"attention_mask\"]\n",
        "            model_kwargs[\"attention_mask\"] = torch.cat(\n",
        "                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
        "            )\n",
        "\n",
        "        if (\n",
        "            model_kwargs.get(\"use_cache\", True)\n",
        "            and \"cache_position\" in model_kwargs\n",
        "            and model_kwargs[\"cache_position\"] is not None\n",
        "        ):\n",
        "            model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + num_new_tokens\n",
        "\n",
        "        return model_kwargs\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_inputs_for_generation(\n",
        "        input_ids: torch.Tensor, past_key_values=None, **kwargs\n",
        "    ) -> Dict[str, Any]:\n",
        "        # Omit tokens covered by past_key_values\n",
        "        if past_key_values:\n",
        "            past_length = past_key_values[0][0].shape[2]\n",
        "\n",
        "            # Some generation methods already pass only the last input ID\n",
        "            if input_ids.shape[1] > past_length:\n",
        "                remove_prefix_length = past_length\n",
        "            else:\n",
        "                # Default to old behavior: keep only final ID\n",
        "                remove_prefix_length = input_ids.shape[1] - 1\n",
        "\n",
        "            input_ids = input_ids[:, remove_prefix_length:]\n",
        "\n",
        "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
        "        position_ids = kwargs.get(\"position_ids\", None)\n",
        "\n",
        "        if attention_mask is not None and position_ids is None:\n",
        "            # create position_ids on the fly for batch generation\n",
        "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "            if past_key_values:\n",
        "                position_ids = position_ids[:, -input_ids.shape[1]:]\n",
        "        else:\n",
        "            position_ids = None\n",
        "\n",
        "        model_inputs = {\n",
        "            'input_ids': input_ids,\n",
        "            \"past_key_values\": past_key_values,\n",
        "            'pixel_values': kwargs['pixel_values'],\n",
        "            'use_cache': kwargs.get(\"use_cache\"),\n",
        "            'labels': kwargs.get(\"labels\"),\n",
        "            'attention_mask': attention_mask,\n",
        "            'position_ids': position_ids\n",
        "        }\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_initial_cache_position(input_ids, model_kwargs):\n",
        "        if not model_kwargs.get(\"use_cache\", True):\n",
        "            model_kwargs[\"cache_position\"] = None\n",
        "            return model_kwargs\n",
        "\n",
        "        model_kwargs[\"cache_position\"] = torch.arange(0, input_ids.shape[-1], device=input_ids.device)\n",
        "        return model_kwargs\n",
        "\n",
        "    @staticmethod\n",
        "    def _expand_inputs_for_generation(\n",
        "        input_ids: Optional[torch.LongTensor],\n",
        "        expand_size: int = 1,\n",
        "        **model_kwargs,\n",
        "    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
        "        def _expand_dict_for_generation(dict_to_expand):\n",
        "            for key in dict_to_expand:\n",
        "                if (\n",
        "                        key != \"cache_position\"\n",
        "                        and dict_to_expand[key] is not None\n",
        "                        and isinstance(dict_to_expand[key], torch.Tensor)\n",
        "                ):\n",
        "                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n",
        "            return dict_to_expand\n",
        "\n",
        "        input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
        "        model_kwargs = _expand_dict_for_generation(model_kwargs)\n",
        "\n",
        "        return input_ids, model_kwargs\n"
      ],
      "metadata": {
        "id": "bn95LwxAll5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import wandb\n",
        "import torchmetrics\n",
        "\n",
        "recognition_model_path = '/content/drive/MyDrive/Models/words-yolov5/epoch=0-step=5468.ckpt'\n",
        "\n",
        "yolo_model = YOLO('/content/yolo_model/best.pt')\n",
        "yolo_model.overrides['data'] = '/content/yolo_model/yolo_config.yaml'\n",
        "\n",
        "config = DTrOCRConfig()\n",
        "test_processor = DTrOCRProcessor(config)\n",
        "\n",
        "recognition_model = DTrOCRLMHeadModel.load_from_checkpoint(recognition_model_path, config=config)\n",
        "recognition_model.to(\"cuda:0\")\n",
        "predicted_pages = []\n",
        "for img_path in test_pages[:2]:\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    results = yolo_model.predict(source=img, imgsz=946, device='cuda:0', conf=0.32, iou=0.55, max_det=500)\n",
        "    results = results[0]\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    img_id = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    predicted_contents = []\n",
        "\n",
        "    for box in results.boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "        class_id = int(box.cls[0])\n",
        "        word_image = img[y1:y2, x1:x2]\n",
        "\n",
        "        inputs = test_processor(\n",
        "          images=word_image,\n",
        "          texts=test_processor.tokeniser.bos_token,\n",
        "          return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        inputs = DTrOCRProcessorOutput(\n",
        "            pixel_values=inputs.pixel_values.to(recognition_model.device),\n",
        "            input_ids=inputs.input_ids.to(recognition_model.device),\n",
        "            attention_mask=inputs.attention_mask.to(recognition_model.device),\n",
        "            labels=inputs.labels\n",
        "        )\n",
        "\n",
        "        model_output = recognition_model.generate(\n",
        "            inputs,\n",
        "            test_processor,\n",
        "            num_beams=3\n",
        "        )\n",
        "\n",
        "        predicted_text = test_processor.tokeniser.decode(model_output[0], skip_special_tokens=True)\n",
        "        predicted_contents.append(predicted_text)\n",
        "\n",
        "    predicted_pages.append(predicted_contents)\n"
      ],
      "metadata": {
        "id": "cThkM02nrUM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_pages)"
      ],
      "metadata": {
        "id": "Zmu6zKNpvEGV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}