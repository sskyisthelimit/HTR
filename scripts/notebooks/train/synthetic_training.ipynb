{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:20:42.195339Z","iopub.status.busy":"2024-09-24T13:20:42.195025Z","iopub.status.idle":"2024-09-24T13:20:57.703374Z","shell.execute_reply":"2024-09-24T13:20:57.702289Z","shell.execute_reply.started":"2024-09-24T13:20:42.195306Z"},"trusted":true},"outputs":[],"source":["!pip install tqdm lightning torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:20:57.706303Z","iopub.status.busy":"2024-09-24T13:20:57.705505Z","iopub.status.idle":"2024-09-24T13:21:02.293358Z","shell.execute_reply":"2024-09-24T13:21:02.292417Z","shell.execute_reply.started":"2024-09-24T13:20:57.706256Z"},"trusted":true},"outputs":[],"source":["import torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:02.295196Z","iopub.status.busy":"2024-09-24T13:21:02.294622Z","iopub.status.idle":"2024-09-24T13:21:02.307193Z","shell.execute_reply":"2024-09-24T13:21:02.306087Z","shell.execute_reply.started":"2024-09-24T13:21:02.295147Z"},"trusted":true},"outputs":[],"source":["from typing import Optional, Union, Tuple, List, Literal\n","\n","\n","class DTrOCRConfig:\n","    def __init__(\n","        self,\n","        gpt2_hf_model: str = 'openai-community/gpt2',\n","        vit_hf_model: str = 'google/vit-base-patch16-224',\n","        vocab_size: Optional[int] = 50257,\n","        max_position_embeddings: Optional[int] = 256,\n","        hidden_size: Optional[int] = 768,\n","        num_hidden_layers: Optional[int] = 8,\n","        num_attention_heads: Optional[int] = 12,\n","        patch_size: Optional[Union[Tuple[int], List[int]]] = (4, 8),      # (height, width)\n","        image_size: Optional[Union[Tuple[int], List[int]]] = (32, 128),   # (height, width)\n","        num_channels: Optional[int] = 3,\n","        resid_pdrop: Optional[float] = 0.1,\n","        embd_pdrop: Optional[float] = 0.1,\n","        attn_pdrop: Optional[float] = 0.1,\n","        layer_norm_epsilon: Optional[float] = 1e-5,\n","        attn_implementation: Literal['sdpa', 'flash_attention_2'] = 'sdpa'\n","    ):\n","        self.gpt2_hf_model = gpt2_hf_model\n","        self.vit_hf_model = vit_hf_model\n","        self.hidden_size = hidden_size\n","        self.num_hidden_layers = num_hidden_layers\n","        self.num_attention_heads = num_attention_heads\n","        self.patch_size = patch_size\n","        self.image_size = image_size\n","        self.num_channels = num_channels\n","        self.vocab_size = vocab_size\n","        self.max_position_embeddings = max_position_embeddings\n","        self.resid_pdrop = resid_pdrop\n","        self.embd_pdrop = embd_pdrop\n","        self.attn_pdrop = attn_pdrop\n","        self.layer_norm_epsilon = layer_norm_epsilon\n","        self._attn_implementation = attn_implementation\n","\n","        # other GPT2 config values\n","        self.n_inner = None\n","        self.scale_attn_weights = True\n","        self.scale_attn_by_inverse_layer_idx = False\n","        self.reorder_and_upcast_attn = False\n","        self.add_cross_attention = False\n","        self.activation_function = \"gelu_new\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:02.309769Z","iopub.status.busy":"2024-09-24T13:21:02.309495Z","iopub.status.idle":"2024-09-24T13:21:02.323404Z","shell.execute_reply":"2024-09-24T13:21:02.322625Z","shell.execute_reply.started":"2024-09-24T13:21:02.309739Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","from dataclasses import dataclass\n","from typing import Optional, Union, List\n","\n","\n","@dataclass\n","class DTrOCRModelOutput:\n","    hidden_states: torch.FloatTensor\n","    past_key_values: torch.FloatTensor\n","\n","\n","@dataclass\n","class DTrOCRLMHeadModelOutput:\n","    logits: torch.FloatTensor\n","    loss: Optional[torch.FloatTensor] = None\n","    accuracy: Optional[torch.FloatTensor] = None\n","    past_key_values: Optional[torch.FloatTensor] = None\n","\n","\n","@dataclass\n","class DTrOCRProcessorOutput:\n","    pixel_values: Optional[torch.FloatTensor] = None\n","    input_ids: Optional[Union[torch.LongTensor, np.ndarray, List[int]]] = None\n","    attention_mask: Optional[Union[torch.FloatTensor, np.ndarray, List[int]]] = None\n","    labels: Optional[Union[torch.LongTensor, np.ndarray, List[int]]] = None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:02.324670Z","iopub.status.busy":"2024-09-24T13:21:02.324403Z","iopub.status.idle":"2024-09-24T13:21:15.469885Z","shell.execute_reply":"2024-09-24T13:21:15.469156Z","shell.execute_reply.started":"2024-09-24T13:21:02.324637Z"},"trusted":true},"outputs":[],"source":["from transformers import GPT2Tokenizer, AutoImageProcessor\n","\n","from PIL import Image\n","from typing import List, Union\n","\n","class DTrOCRProcessor:\n","    def __init__(self, config: DTrOCRConfig, add_bos_token: bool = False, add_eos_token: bool = False):\n","        self.vit_processor = AutoImageProcessor.from_pretrained(\n","            config.vit_hf_model,\n","            size={\n","                \"height\": config.image_size[0],\n","                'width': config.image_size[1]\n","            },\n","            use_fast=True\n","        )\n","        self.tokeniser = GPT2Tokenizer.from_pretrained(\n","            config.gpt2_hf_model,\n","            add_bos_token=add_bos_token,\n","            model_max_length=config.max_position_embeddings - int(\n","                (config.image_size[0] / config.patch_size[0]) * (config.image_size[1] / config.patch_size[1])\n","            )\n","        )\n","        self.tokeniser.pad_token = self.tokeniser.bos_token\n","        self.tokeniser.add_eos_token = add_eos_token\n","\n","        # Bind a new method to gpt2_tokeniser\n","        self.tokeniser.build_inputs_with_special_tokens = modified_build_inputs_with_special_tokens.__get__(\n","            self.tokeniser\n","        )\n","\n","    def __call__(\n","        self,\n","        images: Union[Image.Image, List[Image.Image]] = None,\n","        texts: Union[str, List[str]] = None,\n","        return_labels: bool = False,\n","        input_data_format: str = 'channels_last',\n","        padding: Union[bool, str] = False,\n","        *args,\n","        **kwargs\n","    ) -> DTrOCRProcessorOutput:\n","        text_inputs = self.tokeniser(\n","            texts, padding=padding, *args, **kwargs\n","        ) if texts is not None else None\n","\n","        image_inputs = self.vit_processor(\n","            images, input_data_format=input_data_format, *args, **kwargs\n","        ) if images is not None else None\n","\n","        return DTrOCRProcessorOutput(\n","            pixel_values=image_inputs[\"pixel_values\"] if images is not None else None,\n","            input_ids=text_inputs['input_ids'] if texts is not None else None,\n","            attention_mask=text_inputs['attention_mask'] if texts is not None else None,\n","            labels=text_inputs['input_ids'] if texts is not None and return_labels else None\n","        )\n","\n","\n","def modified_build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","    if self.add_bos_token:\n","        bos_token_ids = [self.bos_token_id]\n","    else:\n","        bos_token_ids = []\n","\n","    if self.add_eos_token:\n","        eos_token_ids = [self.eos_token_id]\n","    else:\n","        eos_token_ids = []\n","\n","    output = bos_token_ids + token_ids_0 + eos_token_ids\n","\n","    if token_ids_1 is None:\n","        return output\n","\n","    return output + bos_token_ids + token_ids_1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:15.471839Z","iopub.status.busy":"2024-09-24T13:21:15.471344Z","iopub.status.idle":"2024-09-24T13:21:16.235375Z","shell.execute_reply":"2024-09-24T13:21:16.234446Z","shell.execute_reply.started":"2024-09-24T13:21:15.471807Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn, Tensor\n","from typing import Optional, Tuple, Dict, Any\n","\n","import pytorch_lightning as pl\n","\n","from transformers.models.vit.modeling_vit import ViTPatchEmbeddings\n","from transformers.generation.logits_process import LogitsProcessorList\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Model\n","from transformers.generation.configuration_utils import GenerationConfig\n","from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask_for_sdpa\n","from transformers.generation.beam_search import BeamScorer, BeamSearchScorer\n","from transformers.generation.stopping_criteria import (\n","    EosTokenCriteria,\n","    MaxLengthCriteria,\n","    MaxTimeCriteria,\n","    StoppingCriteriaList,\n","    StopStringCriteria,\n",")\n","\n","class DTrOCRModel(pl.LightningModule):\n","    def __init__(self, config: DTrOCRConfig):\n","        super().__init__()\n","        # embeddings\n","        self.patch_embeddings = ViTPatchEmbeddings(config)\n","        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n","        self.positional_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n","\n","        self.hidden_layers = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n","        self.dropout = nn.Dropout(config.attn_pdrop)\n","        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n","\n","        self._attn_implementation = config._attn_implementation\n","\n","        # initialise GPT-2 weights from Hugging Face\n","        self.initialise_weights(config)\n","\n","    def forward(\n","        self,\n","        pixel_values: torch.Tensor,\n","        input_ids: torch.LongTensor,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        use_cache: Optional[bool] = False,\n","    ) -> DTrOCRModelOutput:\n","        device = input_ids.device if input_ids is not None else input_ids.device\n","        input_ids = input_ids.view(-1, input_ids.shape[-1])\n","\n","        # past key values\n","        if past_key_values is None:\n","            past_length = 0\n","            past_key_values = tuple([None] * len(self.hidden_layers))\n","        else:\n","            past_length = past_key_values[0][0].size(-2)\n","\n","        patch_embeddings = self.patch_embeddings(pixel_values) if past_length == 0 else None\n","        token_embeddings = self.token_embedding(input_ids)\n","\n","        if patch_embeddings is not None:\n","            patch_and_token_embeddings = torch.concat([patch_embeddings, token_embeddings], dim=-2)\n","        else:\n","            patch_and_token_embeddings = token_embeddings\n","        input_shape = patch_and_token_embeddings.shape\n","\n","        if position_ids is None or past_length == 0:\n","            position_ids = torch.arange(past_length, input_shape[1] + past_length, dtype=torch.long, device=device)\n","            position_ids = position_ids.unsqueeze(0)\n","        else:\n","            position_ids = torch.ones_like(position_ids, device=position_ids.device) * past_length\n","        position_embeddings = self.positional_embedding(position_ids)\n","\n","        hidden_states = patch_and_token_embeddings + position_embeddings\n","        hidden_states = self.dropout(hidden_states)\n","\n","        # attention mask\n","        if attention_mask is not None:\n","            attention_mask = torch.concat(\n","                [\n","                    torch.ones(\n","                        attention_mask.shape[0],\n","                        patch_embeddings.shape[-2] if patch_embeddings is not None else past_length,\n","                        dtype=attention_mask.dtype,\n","                        device=attention_mask.device\n","                    ),\n","                    attention_mask\n","                ], dim=-1\n","            )\n","            if self._attn_implementation == \"flash_attention_2\":\n","                attention_mask = attention_mask if 0 in attention_mask else None\n","            else:\n","                attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n","                    attention_mask=attention_mask,\n","                    input_shape=(input_shape[0], input_shape[-2]),\n","                    inputs_embeds=patch_and_token_embeddings,\n","                    past_key_values_length=past_length,\n","                )\n","\n","        presents = () if use_cache else None\n","        for hidden_layer, layer_past in zip(self.hidden_layers, past_key_values):\n","            outputs = hidden_layer(\n","                hidden_states,\n","                layer_past=layer_past,\n","                attention_mask=attention_mask,\n","                use_cache=use_cache\n","            )\n","            hidden_states = outputs[0]\n","            if use_cache is True:\n","                presents = presents + (outputs[1],)\n","\n","        hidden_states = self.layer_norm(hidden_states)\n","\n","        return DTrOCRModelOutput(hidden_states=hidden_states, past_key_values=presents)\n","\n","    def initialise_weights(self, config: DTrOCRConfig) -> None:\n","        # load pre-trained GPT-2\n","        pretrained_gpt2 = GPT2Model.from_pretrained(config.gpt2_hf_model)\n","\n","        # copy hidden layer weights\n","        for hidden_layer, pretrained_hidden_layer in zip(self.hidden_layers, pretrained_gpt2.h):\n","            hidden_layer.load_state_dict(pretrained_hidden_layer.state_dict())\n","\n","        # token embeddings\n","        self.token_embedding.load_state_dict(pretrained_gpt2.wte.state_dict())\n","\n","        \n","class DTrOCRLMHeadModel(pl.LightningModule):\n","    def __init__(self, config: DTrOCRConfig):\n","        super().__init__()\n","        self.config = config\n","\n","        self.transformer = DTrOCRModel(config)\n","        self.language_model_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n","\n","        image_size, patch_size = config.image_size, config.patch_size\n","\n","        self.image_embedding_length = int((image_size[0] / patch_size[0]) * (image_size[1] / patch_size[1]))\n","        \n","        self.val_cer = torchmetrics.text.CharErrorRate()\n","        self.test_cer = torchmetrics.text.CharErrorRate()\n","\n","    def training_step(self, inputs):\n","        outputs = self.forward(**inputs)\n","        loss = outputs.loss\n","\n","        self.log_dict(\n","            {\n","                \"train_loss\": loss,\n","            },\n","            on_step=False,\n","            on_epoch=True,\n","            prog_bar=True,\n","        )\n","        \n","        return {\"loss\": loss, \"scores\": outputs.accuracy, \"y\": outputs.logits}\n","    \n","    def validation_step(self, inputs):\n","        outputs = self.forward(**inputs)\n","        \n","        labels = inputs['labels']\n","        logits = outputs.logits\n","        \n","        loss = outputs.loss\n","        self.log_dict(\n","            {\n","                \"val_loss\": loss,\n","                \"val_accuracy\": outputs.accuracy\n","            },\n","            on_step=True,\n","            on_epoch=True,\n","            prog_bar=True,\n","        )\n","        \n","        preds = torch.argmax(logits, dim=-1)\n","        self.val_cer.update(preds, labels)\n","        \n","        return loss\n","\n","    def test_step(self, inputs):\n","        outputs = self.forward(**inputs)\n","        loss = outputs.loss\n","        \n","        labels = inputs['labels']\n","        logits = outputs.logits\n","        \n","        self.log_dict(\n","            {\n","                \"val_loss\": loss,\n","                \"val_accuracy\": outputs.accuracy\n","            },\n","            on_step=True,\n","            on_epoch=True,\n","            prog_bar=True,\n","        )\n","        \n","        \n","        preds = torch.argmax(logits, dim=-1)\n","        self.test_cer.update(preds, labels)\n","        \n","        return {\"loss\": loss, \"y\": outputs.logits}\n","    \n","    def on_validation_epoch_end(self):\n","        val_cer_value = self.val_cer.compute()\n","        self.log('val_cer', val_cer_value, prog_bar=True)\n","\n","        self.val_cer.reset()\n","\n","    def on_test_epoch_end(self):\n","        test_cer_value = self.test_cer.compute()\n","        self.log('test_cer', test_cer_value, prog_bar=True)\n","\n","        self.test_cer.reset()\n","        \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(params=self.parameters(), lr=1e-4)\n","\n","    def forward(\n","        self,\n","        pixel_values: torch.Tensor,\n","        input_ids: torch.LongTensor,\n","        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        use_cache: Optional[bool] = False,\n","        labels: Optional[torch.LongTensor] = None,\n","    ) -> DTrOCRLMHeadModelOutput:\n","        transformer_output = self.transformer(\n","            pixel_values=pixel_values,\n","            input_ids=input_ids,\n","            past_key_values=past_key_values,\n","            position_ids=position_ids,\n","            attention_mask=attention_mask,\n","            use_cache=use_cache\n","        )\n","        logits = self.language_model_head(transformer_output.hidden_states)\n","\n","        loss, accuracy = None, None\n","        if labels is not None:\n","            labels = labels.to(logits.device)\n","\n","            # Shift so that tokens < n predict n\n","            shift_logits = logits[..., self.image_embedding_length:-1, :].contiguous()\n","            shift_labels = labels[..., 1:].contiguous()\n","\n","            loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n","            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","\n","            label_matches = shift_labels.view(-1) == torch.argmax(\n","                torch.nn.functional.softmax(shift_logits.view(-1, shift_logits.size(-1)), dim=-1), dim=-1\n","            )\n","\n","            # reduce loss\n","            if attention_mask is not None:\n","                mask = attention_mask[..., 1:].reshape(-1)\n","\n","                loss = (mask * loss).sum() / mask.sum()\n","                accuracy = (mask * label_matches).sum() / mask.sum()\n","            else:\n","                loss = loss.mean()\n","                accuracy = torch.sum(label_matches) / label_matches.shape[0]\n","\n","        return DTrOCRLMHeadModelOutput(\n","            loss=loss,\n","            logits=logits,\n","            accuracy=accuracy,\n","            past_key_values=transformer_output.past_key_values\n","        )\n","\n","    @torch.no_grad()\n","    def generate(\n","            self,\n","            inputs: DTrOCRProcessorOutput,\n","            processor: DTrOCRProcessor,\n","            num_beams: int = 1,\n","            use_cache: bool = True\n","    ):\n","        # params and configs\n","        batch_size = inputs.input_ids.shape[0]\n","        model_kwargs = {\n","            'pixel_values': inputs.pixel_values,\n","            'attention_mask': inputs.attention_mask,\n","            'use_cache': use_cache\n","        }\n","        generation_config = GenerationConfig(\n","            max_new_tokens=1,\n","            pad_token_id=processor.tokeniser.pad_token_id,\n","            eos_token_id=processor.tokeniser.eos_token_id,\n","            bos_token_id=processor.tokeniser.bos_token_id,\n","            num_beams=num_beams,\n","            max_length=processor.tokeniser.model_max_length\n","        )\n","\n","        # interleave input_ids with `num_beams` additional sequences per batch\n","        input_ids, model_kwargs = self._expand_inputs_for_generation(\n","            input_ids=inputs.input_ids,\n","            expand_size=generation_config.num_beams,\n","            **model_kwargs,\n","        )\n","\n","        # prepare stopping criteria\n","        prepared_stopping_criteria = self._get_stopping_criteria(\n","            generation_config=generation_config,\n","            processor=processor\n","        )\n","\n","        if num_beams > 1:\n","            # prepare beam search scorer\n","            beam_scorer = BeamSearchScorer(\n","                batch_size=batch_size,\n","                num_beams=generation_config.num_beams,\n","                device=inputs.input_ids.device,\n","                length_penalty=generation_config.length_penalty,\n","                do_early_stopping=generation_config.early_stopping,\n","                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n","                max_length=generation_config.max_length,\n","            )\n","\n","            # run beam sample\n","            result = self._beam_search(\n","                input_ids,\n","                beam_scorer,\n","                logits_processor=LogitsProcessorList(),\n","                stopping_criteria=prepared_stopping_criteria,\n","                generation_config=generation_config,\n","                **model_kwargs,\n","            )\n","\n","        elif num_beams == 1:\n","            result = self._sample(\n","                input_ids,\n","                logits_processor=LogitsProcessorList(),\n","                stopping_criteria=prepared_stopping_criteria,\n","                generation_config=generation_config,\n","                **model_kwargs,\n","            )\n","        else:\n","            raise ValueError(\"num_beams must be a positive integer.\")\n","\n","        return result\n","\n","    def _sample(\n","        self,\n","        input_ids: torch.Tensor,\n","        logits_processor: LogitsProcessorList,\n","        stopping_criteria: StoppingCriteriaList,\n","        generation_config: GenerationConfig,\n","        **model_kwargs,\n","    ) -> torch.Tensor:\n","        # init values\n","        pad_token_id = generation_config.pad_token_id\n","        has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n","\n","        # keep track of which sequences are already finished\n","        batch_size = input_ids.shape[0]\n","        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n","        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n","\n","        this_peer_finished = False\n","        while not this_peer_finished:\n","            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n","            outputs = self(**model_inputs)\n","\n","            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first\n","            # iteration (the clone itself is always small)\n","            next_token_logits = outputs.logits[:, -1, :].clone()\n","\n","            # pre-process distribution\n","            next_token_scores = logits_processor(input_ids, next_token_logits)\n","\n","            # token selection\n","            next_tokens = torch.argmax(next_token_scores, dim=-1)\n","\n","            # finished sentences should have their next token be a padding token\n","            if has_eos_stopping_criteria:\n","                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n","\n","            # update generated ids, model inputs, and length for next step\n","            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n","\n","            # update generated ids, model inputs, and length for next step\n","            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs)\n","\n","            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, None)\n","            this_peer_finished = unfinished_sequences.max() == 0\n","\n","            # This is needed to properly delete outputs.logits which may be very large for first iteration\n","            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n","            del outputs\n","\n","        return input_ids\n","\n","    def _beam_search(\n","        self,\n","        input_ids: torch.Tensor,\n","        beam_scorer: BeamScorer,\n","        logits_processor: LogitsProcessorList,\n","        stopping_criteria: StoppingCriteriaList,\n","        generation_config: GenerationConfig,\n","        **model_kwargs,\n","    ) -> torch.Tensor:\n","        # init values\n","        pad_token_id = generation_config.pad_token_id\n","        eos_token_id = generation_config.eos_token_id\n","\n","        batch_size = len(beam_scorer._beam_hyps)\n","        num_beams = beam_scorer.num_beams\n","\n","        batch_beam_size, cur_len = input_ids.shape\n","        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n","\n","        if num_beams * batch_size != batch_beam_size:\n","            raise ValueError(\n","                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n","            )\n","\n","        # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n","        # of the first beam are considered to avoid sampling the exact same tokens across all beams.\n","        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n","        beam_scores[:, 1:] = -1e9\n","        beam_scores = beam_scores.view((batch_size * num_beams,))\n","\n","        this_peer_finished = False\n","        decoder_prompt_len = input_ids.shape[-1]\n","        while not this_peer_finished:\n","            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n","            outputs = self(**model_inputs)\n","\n","            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first\n","            # iteration (the clone itself is always small)\n","            next_token_logits = outputs.logits[:, -1, :].clone()\n","            next_token_scores = nn.functional.log_softmax(\n","                next_token_logits, dim=-1\n","            )  # (batch_size * num_beams, vocab_size)\n","\n","            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n","            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n","                next_token_scores_processed\n","            )\n","\n","            # reshape for beam search\n","            vocab_size = next_token_scores.shape[-1]\n","            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n","\n","            # Beam token selection: pick 1 + eos_token_id.shape[0] next tokens for each beam so we have at least 1\n","            # non eos token per beam.\n","            n_tokens_to_keep = max(2, 1 + 1) * num_beams\n","            next_token_scores, next_tokens = torch.topk(\n","                next_token_scores, n_tokens_to_keep, dim=1, largest=True, sorted=True\n","            )\n","\n","            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n","            next_tokens = next_tokens % vocab_size\n","\n","            # stateless\n","            beam_outputs = beam_scorer.process(\n","                input_ids,\n","                next_token_scores,\n","                next_tokens,\n","                next_indices,\n","                pad_token_id=pad_token_id,\n","                eos_token_id=eos_token_id,\n","                decoder_prompt_len=decoder_prompt_len,\n","            )\n","\n","            beam_scores = beam_outputs[\"next_beam_scores\"]\n","            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n","            beam_idx = beam_outputs[\"next_beam_indices\"]\n","\n","            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n","\n","            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs)\n","\n","            # This is needed to properly delete outputs.logits which may be very large for first iteration\n","            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n","            # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n","            # (that way the memory peak does not include outputs.logits)\n","            del outputs\n","\n","            if model_kwargs.get(\"past_key_values\", None) is not None:\n","                model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n","\n","            # increase cur_len\n","            cur_len = cur_len + 1\n","\n","            if beam_scorer.is_done or all(stopping_criteria(input_ids, None)):\n","                this_peer_finished = True\n","\n","        sequence_outputs = beam_scorer.finalize(\n","            input_ids,\n","            beam_scores,\n","            next_tokens,\n","            next_indices,\n","            pad_token_id=pad_token_id,\n","            eos_token_id=eos_token_id,\n","            max_length=stopping_criteria.max_length,\n","            decoder_prompt_len=decoder_prompt_len,\n","        )\n","\n","        return sequence_outputs[\"sequences\"]\n","\n","    def _get_stopping_criteria(\n","        self,\n","        generation_config: GenerationConfig,\n","        processor: Optional[DTrOCRProcessor] = None,\n","    ) -> StoppingCriteriaList:\n","        criteria = StoppingCriteriaList()\n","        if generation_config.max_length is not None:\n","            max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n","            criteria.append(\n","                MaxLengthCriteria(\n","                    max_length=generation_config.max_length,\n","                    max_position_embeddings=max_position_embeddings,\n","                )\n","            )\n","        if generation_config.max_time is not None:\n","            criteria.append(MaxTimeCriteria(max_time=generation_config.max_time))\n","        if generation_config.stop_strings is not None:\n","            if processor is None:\n","                raise ValueError(\n","                    \"There are one or more stop strings, either in the arguments to `generate` or in the \"\n","                    \"model's generation config, but we could not locate a tokenizer. When generating with \"\n","                    \"stop strings, you must pass the model's tokenizer to the `tokenizer` argument of `generate`.\"\n","                )\n","            criteria.append(StopStringCriteria(\n","                stop_strings=generation_config.stop_strings, tokenizer=processor.tokeniser)\n","            )\n","        if generation_config.eos_token_id is not None:\n","            criteria.append(EosTokenCriteria(eos_token_id=generation_config.eos_token_id))\n","        return criteria\n","\n","    @staticmethod\n","    def _reorder_cache(\n","            past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n","    ) -> tuple[tuple[Tensor, ...], ...]:\n","        \"\"\"\n","        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n","        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n","        beam_idx at every generation step.\n","        \"\"\"\n","        return tuple(\n","            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n","            for layer_past in past_key_values\n","        )\n","\n","    @staticmethod\n","    def _update_model_kwargs_for_generation(\n","        outputs: DTrOCRLMHeadModelOutput,\n","        model_kwargs: Dict[str, Any],\n","        num_new_tokens: int = 1,\n","    ) -> Dict[str, Any]:\n","\n","        # update cache\n","        model_kwargs['past_key_values'] = outputs.past_key_values\n","\n","        # update attention mask\n","        if \"attention_mask\" in model_kwargs:\n","            attention_mask = model_kwargs[\"attention_mask\"]\n","            model_kwargs[\"attention_mask\"] = torch.cat(\n","                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n","            )\n","\n","        if (\n","            model_kwargs.get(\"use_cache\", True)\n","            and \"cache_position\" in model_kwargs\n","            and model_kwargs[\"cache_position\"] is not None\n","        ):\n","            model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + num_new_tokens\n","\n","        return model_kwargs\n","\n","    @staticmethod\n","    def prepare_inputs_for_generation(\n","        input_ids: torch.Tensor, past_key_values=None, **kwargs\n","    ) -> Dict[str, Any]:\n","        # Omit tokens covered by past_key_values\n","        if past_key_values:\n","            past_length = past_key_values[0][0].shape[2]\n","\n","            # Some generation methods already pass only the last input ID\n","            if input_ids.shape[1] > past_length:\n","                remove_prefix_length = past_length\n","            else:\n","                # Default to old behavior: keep only final ID\n","                remove_prefix_length = input_ids.shape[1] - 1\n","\n","            input_ids = input_ids[:, remove_prefix_length:]\n","\n","        attention_mask = kwargs.get(\"attention_mask\", None)\n","        position_ids = kwargs.get(\"position_ids\", None)\n","\n","        if attention_mask is not None and position_ids is None:\n","            # create position_ids on the fly for batch generation\n","            position_ids = attention_mask.long().cumsum(-1) - 1\n","            position_ids.masked_fill_(attention_mask == 0, 1)\n","            if past_key_values:\n","                position_ids = position_ids[:, -input_ids.shape[1]:]\n","        else:\n","            position_ids = None\n","\n","        model_inputs = {\n","            'input_ids': input_ids,\n","            \"past_key_values\": past_key_values,\n","            'pixel_values': kwargs['pixel_values'],\n","            'use_cache': kwargs.get(\"use_cache\"),\n","            'labels': kwargs.get(\"labels\"),\n","            'attention_mask': attention_mask,\n","            'position_ids': position_ids\n","        }\n","\n","        return model_inputs\n","\n","    @staticmethod\n","    def _get_initial_cache_position(input_ids, model_kwargs):\n","        if not model_kwargs.get(\"use_cache\", True):\n","            model_kwargs[\"cache_position\"] = None\n","            return model_kwargs\n","\n","        model_kwargs[\"cache_position\"] = torch.arange(0, input_ids.shape[-1], device=input_ids.device)\n","        return model_kwargs\n","\n","    @staticmethod\n","    def _expand_inputs_for_generation(\n","        input_ids: Optional[torch.LongTensor],\n","        expand_size: int = 1,\n","        **model_kwargs,\n","    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n","        def _expand_dict_for_generation(dict_to_expand):\n","            for key in dict_to_expand:\n","                if (\n","                        key != \"cache_position\"\n","                        and dict_to_expand[key] is not None\n","                        and isinstance(dict_to_expand[key], torch.Tensor)\n","                ):\n","                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n","            return dict_to_expand\n","\n","        input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n","        model_kwargs = _expand_dict_for_generation(model_kwargs)\n","\n","        return input_ids, model_kwargs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:16.237045Z","iopub.status.busy":"2024-09-24T13:21:16.236695Z","iopub.status.idle":"2024-09-24T13:21:19.409650Z","shell.execute_reply":"2024-09-24T13:21:19.408648Z","shell.execute_reply.started":"2024-09-24T13:21:16.236999Z"},"trusted":true},"outputs":[],"source":["import os\n","import csv\n","\n","SYNTHETIC_TRAIN_DIR = '/kaggle/input/htr-synthetic/'\n","SYNTHETIC_TEST_DIR = '/kaggle/input/test-htr-synthetic/'\n","\n","test_words = []\n","train_words = []\n","validation_words = []\n","\n","for idx in range(1, 8):\n","    batch_csv_path = os.path.join(SYNTHETIC_TRAIN_DIR, \"batch_\" + str(idx) + '.csv') \n","    with open(batch_csv_path, 'r') as csv_f:\n","        batch = list(csv.reader(csv_f, delimiter=\",\"))[1:]\n","        #full path for each img\n","        batch_dirname = \"words_\" + \"batch_\" + str(idx)\n","        if idx == 1:\n","            batch_dirname = os.path.join(batch_dirname, batch_dirname)\n","        batch = [[os.path.join(SYNTHETIC_TRAIN_DIR, batch_dirname, l[0]) ,l[1]] for l in batch]\n","        train_words += batch\n","            \n","for idx in range(8, 9):\n","    batch_csv_path = os.path.join(SYNTHETIC_TRAIN_DIR, \"batch_\" + str(idx) + '.csv') \n","    with open(batch_csv_path, 'r') as csv_f:\n","        batch = list(csv.reader(csv_f, delimiter=\",\"))[1:]\n","        #full path for each img\n","        batch_dirname = \"words_\" + \"batch_\" + str(idx)\n","        batch = [[os.path.join(SYNTHETIC_TRAIN_DIR, batch_dirname, l[0]) ,l[1]] for l in batch]\n","        validation_words += batch\n","            \n","for idx in range(1, 2):\n","    batch_csv_path = os.path.join(SYNTHETIC_TEST_DIR, \"test_\" + \"batch_\" + str(idx) + '.csv') \n","    with open(batch_csv_path, 'r') as csv_f:\n","        batch = list(csv.reader(csv_f, delimiter=\",\"))[1:]\n","        #full path for each img\n","        batch = [[os.path.join(SYNTHETIC_TEST_DIR, \"test_\" + \"words_\" + \"batch_\" + str(idx), l[0]) ,l[1]] for l in batch]\n","        test_words += batch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:19.411192Z","iopub.status.busy":"2024-09-24T13:21:19.410849Z","iopub.status.idle":"2024-09-24T13:21:20.163750Z","shell.execute_reply":"2024-09-24T13:21:20.162964Z","shell.execute_reply.started":"2024-09-24T13:21:19.411158Z"},"trusted":true},"outputs":[],"source":["test_words = np.array(test_words)\n","train_words = np.array(train_words)\n","validation_words = np.array(validation_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:20.165121Z","iopub.status.busy":"2024-09-24T13:21:20.164796Z","iopub.status.idle":"2024-09-24T13:21:20.173823Z","shell.execute_reply":"2024-09-24T13:21:20.172985Z","shell.execute_reply.started":"2024-09-24T13:21:20.165088Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","class IAMDataset(Dataset):\n","    def __init__(self, config: DTrOCRConfig, mode='train'):\n","        super(IAMDataset, self).__init__()\n","        if mode == \"train\":\n","            self.words = train_words\n","        if mode == \"test\":\n","            self.words = test_words\n","        if mode == \"validation\":\n","            self.words = validation_words\n","\n","        self.processor = DTrOCRProcessor(config, add_eos_token=True, add_bos_token=True)\n","        \n","    def __len__(self):\n","        return len(self.words)\n","    \n","    def __getitem__(self, item):\n","        inputs = self.processor(\n","            images=Image.open(self.words[item][0]).convert('RGB'),\n","            texts=self.words[item][1],\n","            padding='max_length',\n","            return_labels=True,\n","            return_tensors='pt',\n","\n","        )\n","        return {\n","            'pixel_values': inputs.pixel_values[0],\n","            'input_ids': inputs.input_ids[0],\n","            'attention_mask': inputs.attention_mask[0],\n","            'labels': inputs.labels[0]\n","        }\n","\n","config = DTrOCRConfig()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:20.176779Z","iopub.status.busy":"2024-09-24T13:21:20.176432Z","iopub.status.idle":"2024-09-24T13:21:20.187448Z","shell.execute_reply":"2024-09-24T13:21:20.186491Z","shell.execute_reply.started":"2024-09-24T13:21:20.176745Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","import multiprocessing as mp\n","\n","class DTrOCRDataModule(pl.LightningDataModule):\n","    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 16):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.train_data = IAMDataset(mode='train', config=config)\n","        self.test_data = IAMDataset(mode='test', config=config)\n","        self.validation_data = IAMDataset(mode='validation', config=config)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_data, batch_size=16, shuffle=True, num_workers=mp.cpu_count(), drop_last=True)\n","    \n","    def val_dataloader(self):\n","        return DataLoader(self.validation_data, batch_size=16, shuffle=False, num_workers=mp.cpu_count(), drop_last=True)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_data, batch_size=16, shuffle=False, num_workers=mp.cpu_count(), drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T13:21:20.188796Z","iopub.status.busy":"2024-09-24T13:21:20.188526Z"},"trusted":true},"outputs":[],"source":["torch.set_float32_matmul_precision(\"medium\")\n","\n","from lightning.pytorch.profilers import SimpleProfiler\n","from pytorch_lightning.callbacks import EarlyStopping, Callback\n","import time\n","\n","class MyPrintingCallback(Callback):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def on_train_start(self, trainer, pl_module):\n","        print(\"Starting to train!\")\n","\n","    def on_train_end(self, trainer, pl_module):\n","        print(\"Training is done.\")\n","\n","class TimingCallback(Callback):\n","    def __init__(self):\n","        self.validation_times = []\n","\n","    def on_validation_epoch_start(self, trainer, pl_module):\n","        self.validation_start_time = time.time()\n","\n","    def on_validation_epoch_end(self, trainer, pl_module):\n","        validation_duration = time.time() - self.validation_start_time\n","        self.validation_times.append(validation_duration)\n","        self.log_metric(\"validation_time_epoch\", validation_duration)\n","\n","timing_callback = TimingCallback()\n","\n","dm = DTrOCRDataModule()\n","\n","NUM_EPOCHS = 10\n","ACCELERATOR = \"gpu\"\n","DEVICES = [0, 1]\n","PRECISION = \"bf16-mixed\"\n","\n","checkpoint_path = \"/kaggle/input/notebookfc03b07dda/lightning_logs/version_0/checkpoints/epoch=2-step=12958.ckpt\"\n","model = DTrOCRLMHeadModel.load_from_checkpoint(checkpoint_path, config=config, map_location=torch.device('cpu'))\n","\n","trainer = pl.Trainer(\n","    accumulate_grad_batches=2,\n","    strategy='ddp_notebook',\n","    accelerator=ACCELERATOR,\n","    devices=DEVICES,\n","    min_epochs=1,\n","    max_epochs=NUM_EPOCHS,\n","    precision=PRECISION,\n","    callbacks=[MyPrintingCallback(), EarlyStopping(monitor=\"val_loss\"),\n","               TimingCallback()],\n","    default_root_dir=\"/kaggle/working/\",\n","    max_time=\"00:09:00:00\",\n","    profiler=SimpleProfiler(),\n",")\n","\n","trainer.fit(model, dm)\n","# trainer.validate(model, dm)\n","# trainer.test(model, dm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5752850,"sourceId":9462150,"sourceType":"datasetVersion"},{"datasetId":5753856,"sourceId":9463436,"sourceType":"datasetVersion"},{"sourceId":197959928,"sourceType":"kernelVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
